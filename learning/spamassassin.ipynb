{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63c310a7-9ed6-4a4f-92ef-4abb3bece7a0",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "279b6fb8-bdf3-4167-9b0a-026f81652755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "import os\n",
    "\n",
    "# math and data operations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# save models and scores\n",
    "import joblib\n",
    "\n",
    "MODELS_PATH = os.path.join(\".\", \"_models\", \"spamassassin\")\n",
    "\n",
    "if not os.path.isdir(MODELS_PATH):\n",
    "    os.makedirs(MODELS_PATH)\n",
    "\n",
    "def save_model(model, model_id):\n",
    "    path = os.path.join(MODELS_PATH, model_id)\n",
    "    print(\"Saving \", model_id)\n",
    "    joblib.dump(model, path)\n",
    "\n",
    "def load_model(model_id):\n",
    "    path = os.path.join(MODELS_PATH, model_id)\n",
    "    return joblib.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7593d0-09ed-493b-97e4-9b3c3376cba3",
   "metadata": {},
   "source": [
    "# 2. Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4262511-58a9-41fa-a2d3-05a57f2663a1",
   "metadata": {},
   "source": [
    "## Fetch Spam Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2352c21-c879-4eeb-8636-a1e9de13dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
    "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
    "SPAM_PATH = os.path.join(\"_datasets\", \"spamassassin\")\n",
    "\n",
    "def fetch_spam_data(ham_url=HAM_URL, spam_url=SPAM_URL, spam_path=SPAM_PATH):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "    for filename, url in ((\"ham.tar.bz2\", ham_url), (\"spam.tar.bz2\", spam_url)):\n",
    "        path = os.path.join(spam_path, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=spam_path)\n",
    "        tar_bz2_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85c66425-55c0-401d-8b2c-d23e0a8824d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aeb0eb76-e92a-4158-b8b9-1b3754c6432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\n",
    "SPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\n",
    "\n",
    "# we don't want cmds or .ipynb_checkpoints\n",
    "ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) >= 38]\n",
    "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) >= 38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "44d63583-38fa-49a5-9e40-335e19d232a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ham_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d26f8d92-6ce7-4634-9dd3-3779ba2329a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb95db09-f89c-4d7d-a2ba-ee1a9b5209b8",
   "metadata": {},
   "source": [
    "- We want to get the text of these emails and combine the ham and spam into a DataFrame `spam_data`\n",
    "- Then we want to split the `spam_data` into training and test sets\n",
    "    - We may implement hash splitting later, but currently we just want a quick framework for the project\n",
    "- Afterwards, we will set aside the test set and create a pipeline to convert the email into numerical data (1s and 0s for existence of words in the email)\n",
    "- Then we will test some classifier models (svm, randomforest, knn)\n",
    "- Fine tune the model's inherent hyperparams and get cross-val-scores\n",
    "- Go back to feature engineering and data preparation and possibly include these in a pipeline for fine-tuning\n",
    "- Test till a reasonable accuracy\n",
    "- Evaluate on test set\n",
    "- Analyze errors and potentially repeat steps to create a better model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4802bd-500c-4787-81db-9d1333574e0b",
   "metadata": {},
   "source": [
    "## Get Email Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c2e657b-1d45-45ef-932a-51f22395be20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import email.policy\n",
    "\n",
    "# open and parse a single email file using Python's email library\n",
    "def load_email(path, filename):\n",
    "    with open(os.path.join(path, filename), \"rb\") as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
    "\n",
    "ham_emails = [load_email(HAM_DIR, name) for name in ham_filenames]\n",
    "spam_emails = [load_email(SPAM_DIR, name) for name in spam_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78977c18-e6e0-47c5-827d-40319a8e8e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Date:        Wed, 21 Aug 2002 10:54:46 -0500\n",
      "    From:        Chris Garrigues <cwg-dated-1030377287.06fa6d@DeepEddy.Com>\n",
      "    Message-ID:  <1029945287.4797.TMDA@deepeddy.vircio.com>\n",
      "\n",
      "\n",
      "  | I can't reproduce this error.\n",
      "\n",
      "For me it is very repeatable... (like every time, without fail).\n",
      "\n",
      "This is the debug log of the pick happening ...\n",
      "\n",
      "18:19:03 Pick_It {exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace} {4852-4852 -sequence mercury}\n",
      "18:19:03 exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace 4852-4852 -sequence mercury\n",
      "18:19:04 Ftoc_PickMsgs {{1 hit}}\n",
      "18:19:04 Marking 1 hits\n",
      "18:19:04 tkerror: syntax error in expression \"int ...\n",
      "\n",
      "Note, if I run the pick command by hand ...\n",
      "\n",
      "delta$ pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace  4852-4852 -sequence mercury\n",
      "1 hit\n",
      "\n",
      "That's where the \"1 hit\" comes from (obviously).  The version of nmh I'm\n",
      "using is ...\n",
      "\n",
      "delta$ pick -version\n",
      "pick -- nmh-1.0.4 [compiled on fuchsia.cs.mu.OZ.AU at Sun Mar 17 14:55:56 ICT 2002]\n",
      "\n",
      "And the relevant part of my .mh_profile ...\n",
      "\n",
      "delta$ mhparam pick\n",
      "-seq sel -list\n",
      "\n",
      "\n",
      "Since the pick command works, the sequence (actually, both of them, the\n",
      "one that's explicit on the command line, from the search popup, and the\n",
      "one that comes from .mh_profile) do get created.\n",
      "\n",
      "kre\n",
      "\n",
      "ps: this is still using the version of the code form a day ago, I haven't\n",
      "been able to reach the cvs repository today (local routing issue I think).\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________\n",
      "Exmh-workers mailing list\n",
      "Exmh-workers@redhat.com\n",
      "https://listman.redhat.com/mailman/listinfo/exmh-workers\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ham_emails[0].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375025b2-7245-4115-b33a-9c7feac1f31e",
   "metadata": {},
   "source": [
    "## Structure of Email Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5c6ad4-01ba-4e25-856c-685698cbd056",
   "metadata": {},
   "source": [
    "Emails can have different structures, this is included in the payload of the email object. Let's create functions for getting and counting the structures of emails and how many emails are of that structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4b00a6e4-4839-4497-9433-7d491b52995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            get_email_structure(sub_email)\n",
    "            for sub_email in payload\n",
    "        ]))\n",
    "    else:\n",
    "        return email.get_content_type()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061d20f5-16d2-43b6-a0f9-c84b918282a1",
   "metadata": {},
   "source": [
    "Now lets create a function for getting all structures and counting how many occurences of each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0709ca1-4a57-431f-b76e-c49b9e801fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f3f0dddd-8809-4ab7-a213-b91e0a62f3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 2408),\n",
       " ('multipart(text/plain, application/pgp-signature)', 66),\n",
       " ('multipart(text/plain, text/html)', 8),\n",
       " ('multipart(text/plain, text/plain)', 4),\n",
       " ('multipart(text/plain)', 3),\n",
       " ('multipart(text/plain, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, text/enriched)', 1),\n",
       " ('multipart(text/plain, application/ms-tnef, text/plain)', 1),\n",
       " ('multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)',\n",
       "  1),\n",
       " ('multipart(text/plain, video/mng)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain))', 1),\n",
       " ('multipart(text/plain, application/x-pkcs7-signature)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)',\n",
       "  1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))',\n",
       "  1),\n",
       " ('multipart(text/plain, application/x-java-applet)', 1)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_structures = structures_counter(ham_emails).most_common()\n",
    "\n",
    "ham_structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "44709bb3-0dc0-42ec-9d32-8cca2d324b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 218),\n",
       " ('text/html', 183),\n",
       " ('multipart(text/plain, text/html)', 45),\n",
       " ('multipart(text/html)', 20),\n",
       " ('multipart(text/plain)', 19),\n",
       " ('multipart(multipart(text/html))', 5),\n",
       " ('multipart(text/plain, image/jpeg)', 3),\n",
       " ('multipart(text/html, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, application/octet-stream)', 1),\n",
       " ('multipart(text/html, text/plain)', 1),\n",
       " ('multipart(multipart(text/html), application/octet-stream, image/jpeg)', 1),\n",
       " ('multipart(multipart(text/plain, text/html), image/gif)', 1),\n",
       " ('multipart/alternative', 1)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_structures = structures_counter(spam_emails).most_common()\n",
    "\n",
    "spam_structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65993137-bed7-43ce-b829-caf75135db86",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "Insights from email's **structure**\n",
    "- spam has a lot more html texts and is never signed with a pgp-signature\n",
    "- we can use structure as a valuable feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b9f7ee-1064-45cc-86c9-301baaa70725",
   "metadata": {},
   "source": [
    "## Create Training and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550e5b43-73c5-4e77-858c-fdb7dc4985db",
   "metadata": {},
   "source": [
    "We will use train_test_split to be fast. There are other ways for reproducibility and preserving train, test set integrity while adding new data\n",
    "* Combine ham and spam emails into `X`\n",
    "* Create similar length label arrays with labels 0 for ham and 1 for spam - `y`\n",
    "* Apply train_test_split and get `X_train`, `X_test`, `y_train`, `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ffa7fd2a-1a6f-4997-9da0-9243c448d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(ham_emails + spam_emails, dtype=object)\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0a4a1d48-ff6d-4b31-b45a-74a3ac6d0520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2400,), (600,), (2400,), (600,))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9240dae-e346-4757-9b5d-edd0564bf099",
   "metadata": {},
   "source": [
    "# 3. Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b00b5-ccd1-4b8f-b1c1-90086a6b86b1",
   "metadata": {},
   "source": [
    "## Convert Email to Plain Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8333ce93-e210-4550-8caa-b9c2135d93d2",
   "metadata": {},
   "source": [
    "The follow function converts **HTML to plain text**\n",
    "- Drops `<head>` section\n",
    "- Converts all `<a>` tags to text HYPERLINK\n",
    "- Gets rid of all HTML tags\n",
    "- Replaces multiple newlines with single newline\n",
    "- Unescapes HTML entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "51e8930b-4294-4699-8747-e56930963f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from html import unescape\n",
    "\n",
    "def html_to_plain_text(html):\n",
    "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "    return unescape(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c15deb-8b2f-4f05-8aaa-52f402245e82",
   "metadata": {},
   "source": [
    "The following is html spam email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "62471391-3d93-4ad4-bc13-522b56f37e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HTML><HEAD><TITLE></TITLE><META http-equiv=\"Content-Type\" content=\"text/html; charset=windows-1252\"><STYLE>A:link {TEX-DECORATION: none}A:active {TEXT-DECORATION: none}A:visited {TEXT-DECORATION: none}A:hover {COLOR: #0033ff; TEXT-DECORATION: underline}</STYLE><META content=\"MSHTML 6.00.2713.1100\" name=\"GENERATOR\"></HEAD>\n",
      "<BODY text=\"#000000\" vLink=\"#0033ff\" link=\"#0033ff\" bgColor=\"#CCCC99\"><TABLE borderColor=\"#660000\" cellSpacing=\"0\" cellPadding=\"0\" border=\"0\" width=\"100%\"><TR><TD bgColor=\"#CCCC99\" valign=\"top\" colspan=\"2\" height=\"27\">\n",
      "<font size=\"6\" face=\"Arial, Helvetica, sans-serif\" color=\"#660000\">\n",
      "<b>OTC</b></font></TD></TR><TR><TD height=\"2\" bgcolor=\"#6a694f\">\n",
      "<font size=\"5\" face=\"Times New Roman, Times, serif\" color=\"#FFFFFF\">\n",
      "<b>&nbsp;Newsletter</b></font></TD><TD height=\"2\" bgcolor=\"#6a694f\"><div align=\"right\"><font color=\"#FFFFFF\">\n",
      "<b>Discover Tomorrow's Winners&nbsp;</b></font></div></TD></TR><TR><TD height=\"25\" colspan=\"2\" bgcolor=\"#CCCC99\"><table width=\"100%\" border=\"0\"  ...\n"
     ]
    }
   ],
   "source": [
    "html_spam_emails = [email for email in X_train[y_train==1]\n",
    "                    if get_email_structure(email) == \"text/html\"]\n",
    "sample_html_spam = html_spam_emails[7]\n",
    "print(sample_html_spam.get_content().strip()[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89ef32d-3873-47ed-8b98-73fcc876fa4d",
   "metadata": {},
   "source": [
    "***\n",
    "Same email converted to plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7161c357-0f05-4a00-814c-4e8a4187b189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OTC\n",
      " Newsletter\n",
      "Discover Tomorrow's Winners \n",
      "For Immediate Release\n",
      "Cal-Bay (Stock Symbol: CBYI)\n",
      "Watch for analyst \"Strong Buy Recommendations\" and several advisory newsletters picking CBYI.  CBYI has filed to be traded on the OTCBB, share prices historically INCREASE when companies get listed on this larger trading exchange. CBYI is trading around 25 cents and should skyrocket to $2.66 - $3.25 a share in the near future.\n",
      "Put CBYI on your watch list, acquire a position TODAY.\n",
      "REASONS TO INVEST IN CBYI\n",
      "A profitable company and is on track to beat ALL earnings estimates!\n",
      "One of the FASTEST growing distributors in environmental & safety equipment instruments.\n",
      "Excellent management team, several EXCLUSIVE contracts.  IMPRESSIVE client list including the U.S. Air Force, Anheuser-Busch, Chevron Refining and Mitsubishi Heavy Industries, GE-Energy & Environmental Research.\n",
      "RAPIDLY GROWING INDUSTRY\n",
      "Industry revenues exceed $900 million, estimates indicate that there could be as much as $25 billi ...\n"
     ]
    }
   ],
   "source": [
    "print(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b6ac30-39b2-48d7-9a4c-dd2d4665e04b",
   "metadata": {},
   "source": [
    "***\n",
    "Following function converts email object to plain text no matter the format.\n",
    "- Loops through all parts of email\n",
    "- Returns plain text content\n",
    "- Converts html to plain text and returns resulting content if there isn't plain text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6fa5f26f-8dfd-43b3-a5b6-b93e2d0167e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_text(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except: # in case of encoding issues\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == \"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        return html_to_plain_text(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "de88a5d8-ff50-4426-bb89-de2f18ea8d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OTC\n",
      " Newsletter\n",
      "Discover Tomorrow's Winners \n",
      "For Immediate Release\n",
      "Cal-Bay (Stock Symbol: CBYI)\n",
      "Wat ...\n"
     ]
    }
   ],
   "source": [
    "print(email_to_text(sample_html_spam)[:100], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab54f27f-e0ac-4e4c-bf22-d615f207769d",
   "metadata": {},
   "source": [
    "## Stemming using NLTK Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4da2143-acbf-4397-9139-639f878d98d1",
   "metadata": {},
   "source": [
    "You will need to install nltk\n",
    "- `pip install nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6300e3f3-3922-487f-a9b1-7b5a9852e725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations => comput\n",
      "Computation => comput\n",
      "Computing => comput\n",
      "Computed => comput\n",
      "Compute => comput\n",
      "Compulsive => compuls\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import nltk\n",
    "\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n",
    "        print(word, \"=>\", stemmer.stem(word))\n",
    "except ImportError:\n",
    "    print(\"Error: stemming requires the NLTK module.\")\n",
    "    stemmer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19bb812-f2bc-4e4b-bcaf-cd639da530b4",
   "metadata": {},
   "source": [
    "## Replace urls with text URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc3e708-9b3e-4d4d-8332-c084505f1b1f",
   "metadata": {},
   "source": [
    "We will use library urlextract to replace urls with \"URL\"\n",
    "- `pip install urlextract`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "868ae9ae-4d14-48c4-b8e5-c62066a248a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['github.com', 'https://youtu.be/7Pq-S557XQU?t=3m32s']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import urlextract # may require an Internet connection to download root domain names\n",
    "    \n",
    "    url_extractor = urlextract.URLExtract()\n",
    "    print(url_extractor.find_urls(\"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s\"))\n",
    "except ImportError:\n",
    "    print(\"Error: replacing URLs requires the urlextract module.\")\n",
    "    url_extractor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a916cf4-f7a5-45b3-a1b4-3ab750ee9e69",
   "metadata": {},
   "source": [
    "## Transform Email to Array of Word Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b66275e-248c-478e-bb38-908d604237b5",
   "metadata": {},
   "source": [
    "The following code converts an array of email objects into an array of counter objects that track word count in the email.\n",
    "- Loops through each email in array of emails\n",
    "- Transforms the email to text\n",
    "- Counts the words using Counter\n",
    "- Append each counter to list\n",
    "- After looping through all emails, return list of counters as array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3a9a6e5a-c2d5-4f9f-8315-eb8be555107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n",
    "                 replace_urls=True, replace_numbers=True, stemming=True):\n",
    "        self.strip_headers = strip_headers\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.stemming = stemming\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            text = email_to_text(email) or \"\"\n",
    "            if self.lower_case:\n",
    "                text = text.lower()\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            word_counts = Counter(text.split())\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff10610-b076-4090-bb7b-6b68c0423a1c",
   "metadata": {},
   "source": [
    "Let's test this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f48290d2-9846-42d2-b68a-b3fa95331a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Some interesting quotes...\n",
      "\n",
      "http://www.postfun.com/pfp/worbois.html\n",
      "\n",
      "\n",
      "Thomas Jefferson:\n",
      "\n",
      "\"I have examined all the known superstitions of the word, and I do not\n",
      "find in our particular superstition of Christianity one redeeming feature.\n",
      "They are all alike founded on fables and mythology. Millions of innocent\n",
      "men, women and children, since the introduction of Christianity, have been\n",
      "burnt, tortured, fined and imprisoned. What has been the effect of this\n",
      "coercion? To make one half the world fools and the other half hypocrites;\n",
      "to support roguery and error all over the earth.\"\n",
      "\n",
      "SIX HISTORIC AMERICANS,\n",
      "by John E. Remsburg, letter to William Short\n",
      "Jefferson again:\n",
      "\n",
      "\"Christianity...(has become) the most perverted system that ever shone on\n",
      "man. ...Rogueries, absurdities and untruths were perpetrated upon the\n",
      "teachings of Jesus by a large band of dupes and importers led by Paul, the\n",
      "first great corrupter of the teaching of Jesus.\"\n",
      "\n",
      "\n",
      "\n",
      "Counter({'the': 11, 'of': 9, 'and': 8, 'all': 3, 'christian': 3, 'to': 3, 'by': 3, 'jefferson': 2, 'i': 2, 'have': 2, 'superstit': 2, 'one': 2, 'on': 2, 'been': 2, 'ha': 2, 'half': 2, 'rogueri': 2, 'teach': 2, 'jesu': 2, 'some': 1, 'interest': 1, 'quot': 1, 'url': 1, 'thoma': 1, 'examin': 1, 'known': 1, 'word': 1, 'do': 1, 'not': 1, 'find': 1, 'in': 1, 'our': 1, 'particular': 1, 'redeem': 1, 'featur': 1, 'they': 1, 'are': 1, 'alik': 1, 'found': 1, 'fabl': 1, 'mytholog': 1, 'million': 1, 'innoc': 1, 'men': 1, 'women': 1, 'children': 1, 'sinc': 1, 'introduct': 1, 'burnt': 1, 'tortur': 1, 'fine': 1, 'imprison': 1, 'what': 1, 'effect': 1, 'thi': 1, 'coercion': 1, 'make': 1, 'world': 1, 'fool': 1, 'other': 1, 'hypocrit': 1, 'support': 1, 'error': 1, 'over': 1, 'earth': 1, 'six': 1, 'histor': 1, 'american': 1, 'john': 1, 'e': 1, 'remsburg': 1, 'letter': 1, 'william': 1, 'short': 1, 'again': 1, 'becom': 1, 'most': 1, 'pervert': 1, 'system': 1, 'that': 1, 'ever': 1, 'shone': 1, 'man': 1, 'absurd': 1, 'untruth': 1, 'were': 1, 'perpetr': 1, 'upon': 1, 'a': 1, 'larg': 1, 'band': 1, 'dupe': 1, 'import': 1, 'led': 1, 'paul': 1, 'first': 1, 'great': 1, 'corrupt': 1})\n"
     ]
    }
   ],
   "source": [
    "X_few = X_train[:3]\n",
    "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
    "print(X_few[1].get_content())\n",
    "print(X_few_wordcounts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c05b782-8e9b-4670-8fac-13ca50adca07",
   "metadata": {},
   "source": [
    "## Converting Word Counts to Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc6ac51-9786-4f2e-ab6c-fdc4d1d60f2d",
   "metadata": {},
   "source": [
    "In the code below\n",
    "- fit creates a vocabulary of the top 1000 (default) words from word count array\n",
    "- transform creates a sparse matrix or 2D array \n",
    "    - rows are instances\n",
    "    - columns are the top 1000 most common words\n",
    "    - values are the word counts of the column it belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b92f4069-9e54-4e5d-b89b-36244553ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        # list of most common words\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        # dictionary of word: index + 1 (so it starts at 1 for most common word)\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        # enumerate generates counter for each word_count array in X\n",
    "        for row, word_count in enumerate(X):\n",
    "            # word_count is a Counter() which is a dictionary with elements \"word\": count\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                # appends index or identifier and 0 if the word is not found in top 1000 \n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6fdf7d6c-9f89-401b-8a4b-7fe4f0719967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x11 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 20 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8d8f12b2-b5ab-4ceb-9f08-112de6bb8251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [99, 11,  9,  8,  3,  1,  3,  1,  3,  2,  3],\n",
       "       [67,  0,  1,  2,  3,  4,  1,  2,  0,  1,  0]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421c3dc9-866a-42aa-ad71-6fabb09edede",
   "metadata": {},
   "source": [
    "The first number in the vector is for col 0 aka 6 words are not in the top 10 vocabulary (and those are all the 6 words in the first email).\n",
    "\n",
    "We can look at the vocabulary below to see which numbers correspond to which words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "833fecb4-7a59-494c-a995-85d903ebb259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'of': 2,\n",
       " 'and': 3,\n",
       " 'to': 4,\n",
       " 'url': 5,\n",
       " 'all': 6,\n",
       " 'in': 7,\n",
       " 'christian': 8,\n",
       " 'on': 9,\n",
       " 'by': 10}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5cf7f6-9c8c-4a48-9bde-1bb7eaf72373",
   "metadata": {},
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9594ab-3c24-4a3a-86fd-25009041b061",
   "metadata": {},
   "source": [
    "Now let's make this into a pipeline.\n",
    "- Convert emails into array of word counts Counter objects\n",
    "- Convert word counts objects into vectors of number of instances of top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "95e48506-97db-48d7-8c3b-0375ce7fbcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preparation_pipeline = Pipeline([\n",
    "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer())\n",
    "])\n",
    "\n",
    "X_train_transformed = preparation_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "90d8a12f-ba59-4748-8036-0dd6a7e1423a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 1001)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610abe4d-fc55-4993-b896-9c950979bfb8",
   "metadata": {},
   "source": [
    "2400 instances and 1000 top words and one more column for words that aren't in the top 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e862c-8abf-4c65-b223-ba0c8d4a80e7",
   "metadata": {},
   "source": [
    "# 4. Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71338b46-ed79-49da-b6ba-425a2dbad75f",
   "metadata": {},
   "source": [
    "There are many models that can classify email, but I don't know the more complicated ones, so I will just try out the following:\n",
    "- Logistic Regression\n",
    "- SVM Classifier\n",
    "- Random Forest Classifier\n",
    "- k Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "566c53a4-e414-4a62-a6a1-cd4aaa52b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "svm_clf = SVC(gamma=\"auto\")\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "knn_clf = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b28c2-ca76-4fea-8f1d-b57f4519d69b",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0fddd9fc-1331-453b-8f74-7d9336ba3ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.985"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(log_reg, X_train_prepared, y_train,\n",
    "                         scoring=\"accuracy\", cv=3)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb228767-62df-4aa9-a6e7-01d034ea288f",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a0652209-0784-484f-bdf5-471596dfbc09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9504166666666666"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(svm_clf, X_train_prepared, y_train,\n",
    "                         scoring=\"accuracy\", cv=3)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a96300c-07c8-47bc-ac1c-8036f77e6e48",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "90f50360-1c30-4098-b4f1-eb94bd79182f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9820833333333333"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(forest_clf, X_train_prepared, y_train,\n",
    "                         scoring=\"accuracy\", cv=3)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed65a2ee-1bd0-4625-b0b9-1c7aa5146974",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c491d38b-8e7b-42b7-9563-ec8ac4740b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9208333333333334"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(knn_clf, X_train_prepared, y_train,\n",
    "                         scoring=\"accuracy\", cv=3)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b598d-af7f-48dd-a82f-5a2f47eb1a92",
   "metadata": {},
   "source": [
    "## Precision-Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163d8ea1-03e2-46ed-9340-98ac056006f8",
   "metadata": {},
   "source": [
    "The logistic regression model performed the best, but this is because the dataset was an easy one and wasn't as complex. If it was more complex we would definitely see different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ffc9fa2c-2cb1-41a5-9958-ca98d9e98613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 96.88%\n",
      "Recall: 97.89%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "X_test_transformed = preparation_pipeline.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

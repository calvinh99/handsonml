{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890d6e02-9127-4aaa-905c-cf122376b10f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "926ba2df-f0d5-44d3-82ab-d5da07ab5323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle math and data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "\n",
    "# to plot nice figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# handle files\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "\n",
    "# deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Example protobuf\n",
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49704995-2a95-403d-839e-0199f36c80b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aedc449-a19e-44ee-906c-2da6c95545d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random states\n",
    "SEED = 69\n",
    "\n",
    "K = keras.backend\n",
    "\n",
    "def reset_backend():\n",
    "    K.clear_session()\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "# tf datasets directory\n",
    "DATA_DIR = os.path.join(\".\", \"_datasets\", \"12_tf_data\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# DEBUG mode (aka testing and experimenting)\n",
    "DEBUG = True\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef5e0e9-85d7-4d56-9948-656b0b7fb280",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eace0fb-ac27-4dad-ba4d-4fb79306e86b",
   "metadata": {},
   "source": [
    "## Datasets and Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14a18597-7290-43ac-98d4-2ff5ab38e5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RangeDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_1 = tf.data.Dataset.range(100)\n",
    "dataset_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39cc6c63-19a6-455e-80dd-8914efe00b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_items_in_dataset(dataset):\n",
    "    for item in dataset:\n",
    "        print(item.numpy(), end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bdaa4d08-170a-4f2d-ab3c-ad94d5936aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[10 11 12 13 14 15 16 17 18 19]\n",
      "[20 21 22 23 24 25 26 27 28 29]\n",
      "[30 31 32 33 34 35 36 37 38 39]\n",
      "[40 41 42 43 44 45 46 47 48 49]\n",
      "[50 51 52 53 54 55 56 57 58 59]\n",
      "[60 61 62 63 64 65 66 67 68 69]\n",
      "[70 71 72 73 74 75 76 77 78 79]\n",
      "[80 81 82 83 84 85 86 87 88 89]\n",
      "[90 91 92 93 94 95 96 97 98 99]\n"
     ]
    }
   ],
   "source": [
    "print_items_in_dataset(dataset_1.batch(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd514641-ff09-42b6-963d-bb5a55a3c9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4  9  7  8  3  6 14 10  1 18]\n",
      "[ 5 17 19  2 11 15  0 21 26 12]\n",
      "[22 16 23 13 28 33 25 35 36 31]\n",
      "[37 32 39 29 41 27 24 46 43 34]\n",
      "[49 40 51 20 52 44 53 54 48 57]\n",
      "[50 42 30 38 60 45 56 64 47 68]\n",
      "[55 65 61 70 58 71 75 63 67 73]\n",
      "[78 66 79 59 76 82 77 84 74 85]\n",
      "[69 88 62 81 86 90 93 83 92 89]\n",
      "[72 95 87 99 80 91 98 94 97 96]\n"
     ]
    }
   ],
   "source": [
    "dataset_1_shuff = dataset_1.shuffle(buffer_size=10, seed=SEED).batch(10)\n",
    "print_items_in_dataset(dataset_1_shuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "918242e2-0d11-4089-aa7d-945348923b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74 67 73 50 88 61 44 25 11 31]\n",
      "[ 0 10 35 99 27 70 22 56 64  1]\n",
      "[42 87 43 28 23 24 49 13 72 30]\n",
      "[37 39  2 53 51 55 40 84 83 15]\n",
      "[14 62 32  9 48 82 21 45 18 97]\n",
      "[80 94 46 47 96 57 92 63  8 36]\n",
      "[34 75 26 12 68 76 69 59 29 54]\n",
      "[98  3 79 60 90 33 41 95 86 58]\n",
      "[ 6 81  5 71 66 77 19 38 20 17]\n",
      "[ 7 85 89 78 93 91 16  4 65 52]\n"
     ]
    }
   ],
   "source": [
    "dataset_1_shuff = dataset_1.shuffle(buffer_size=100, seed=SEED).batch(10)\n",
    "print_items_in_dataset(dataset_1_shuff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb6ecb7-6143-484e-9878-63ba999eb6d9",
   "metadata": {},
   "source": [
    "So the batch is what gets random sampled into the returned dataset (the new shuffled dataset), and every time it gets random sampled, the next item in the original dataset will replace what was sampled.\n",
    "\n",
    ">Ex. If batch size is 2 then 0 and 1 will be in batch. Then let's say 1 gets randomly sampled, then new batch will be 0 and 2 since 1 gets replaced by next item. Then this is repeated. \n",
    "\n",
    ">So a small batch size will return a new dataset that is only shuffled a little, since it's hard for 0 or 1 to just happen to not get sampled 90 times in a row, and it is impossible to sample let's say 99 in the beginning, since it is literally the last element.\n",
    "\n",
    ">A batch size equal to size of dataset will basically just be normal shuffling since it will randomly sample from the batch, but the batch is literally the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72700560-01ba-4bed-968e-aac743750350",
   "metadata": {},
   "source": [
    "## Interleaving Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b18ec9-b79f-4877-8a7f-847922ee003b",
   "metadata": {},
   "source": [
    "You shuffle and split the data file into many different files, and then you interleave and read from the files in a random order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af1ba540-8050-49c2-bea9-1c1820a6df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepaths = os.path.join(\".\", \"_datasets\", \"housing\", \"my_housing_*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d2452a5-62d1-4fb6-b7b3-f6090c0ae2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2048a5ad-7b10-472e-9409-8b4e2b8d506a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'./_datasets/housing/my_housing_ag', shape=(), dtype=string)\n",
      "tf.Tensor(b'./_datasets/housing/my_housing_aj', shape=(), dtype=string)\n",
      "tf.Tensor(b'./_datasets/housing/my_housing_af', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for filepath in filepath_dataset.take(3):\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9ab0a374-2471-4d2c-9188-64d7089bdfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "bytestring_dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b1fee396-3e43-4759-99cd-0536cee14d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'-118.2,34.13,52.0,2035.0,459.0,2589.0,438.0,3.5349,193600.0,<1H OCEAN'\n",
      "b'-121.79,36.95,34.0,2152.0,430.0,1516.0,386.0,3.7863,192200.0,<1H OCEAN'\n",
      "b'-121.21,37.8,33.0,1862.0,429.0,971.0,389.0,2.6053,99200.0,INLAND'\n"
     ]
    }
   ],
   "source": [
    "for line in bytestring_dataset.take(3):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c55ed-9d7f-4c5b-9c84-29493bc24ac1",
   "metadata": {},
   "source": [
    "## Parse and Prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "42ac1d4b-5967-445e-8334-0ec9d24d8ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 8\n",
    "\n",
    "def parse(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)] + ['']\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    # drop categorical\n",
    "    fields = fields[:-1]\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6af912b7-e048-4064-a3ee-c0f0f629dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = bytestring_dataset.map(parse, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cf569c2c-43fa-4228-80b3-b73127f7fb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-117.89     34.09     35.     1205.      330.      583.      319.\n",
      "    2.3971] [188900.]\n"
     ]
    }
   ],
   "source": [
    "for tensor in dataset.take(1):\n",
    "    print(tensor[0].numpy(), tensor[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cf92ffc1-53fa-4033-9c5b-9aa7a3bafb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.batch(10).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dab18b97-2a2f-40fb-892c-c7c71f5411d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3fa0ca1f-2a55-4fd1-8e67-b891e45a61c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset element_spec=(TensorSpec(shape=(8,), dtype=tf.float32, name=None), TensorSpec(shape=(1,), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d80c39af-cb3e-49b4-8715-66bd57684180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ParallelInterleaveDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytestring_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e6bdcf74-b326-4cf2-b315-ecb1b9e477c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8161a37d-5310-4f5e-967c-6c8743fd3975",
   "metadata": {},
   "source": [
    "So calling prefetch makes sense, because each dataset just wraps another one - so while you are training on one batch, another thread would read data from shuffled filepaths from disk, interleave the bytestrings, then map the parse function, then create another batch.\n",
    "- Or maybe it's in reverse order: a batch would be determined, then that many lines are shuffled and interleaved and parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c10cfc0-cd81-420b-ae9f-391f01489f87",
   "metadata": {},
   "source": [
    "## Tensorflow Protobufs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e7917f-e708-4ed9-a589-793b13dcbd3c",
   "metadata": {},
   "source": [
    "Protobufs are ways to easily store structured data and serialize and parse the data. This way it allows data to be useful across all languages that can implement tensorflow.\n",
    ">The `Example` protobuf is a chain of protobuf objects that ultimately allow you to store an instance with its feature names and feature values as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "eb9734cb-2c81-4b2b-890c-675121e6d7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example\n",
    "\n",
    "person_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            \"name\": Feature(\n",
    "                bytes_list=BytesList(\n",
    "                    value=[b\"Alice\"]\n",
    "                )\n",
    "            ),\n",
    "            \"id\": Feature(\n",
    "                int64_list=Int64List(\n",
    "                    value=[123]\n",
    "                )\n",
    "            ),\n",
    "            \"emails\": Feature(\n",
    "                bytes_list=BytesList(\n",
    "                    value=[b\"alice123@gmail.com\",\n",
    "                           b\"alicecoolkid@icloud.com\"]\n",
    "                )\n",
    "            )\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d4102b03-bde6-4ef0-ab8f-3170b10ba2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n[\\n\\x0b\\n\\x02id\\x12\\x05\\x1a\\x03\\n\\x01{\\n\\x11\\n\\x04name\\x12\\t\\n\\x07\\n\\x05Alice\\n9\\n\\x06emails\\x12/\\n-\\n\\x12alice123@gmail.com\\n\\x17alicecoolkid@icloud.com'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serialized_person_example = person_example.SerializeToString()\n",
    "serialized_person_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e79daf8-cae0-42fe-83a5-067b77adbb80",
   "metadata": {},
   "source": [
    "Store the protobuf as a tfrecord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "df4d5953-fb40-41a5-b604-7f6ac2fab5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_protobuf_path = os.path.join(DATA_DIR, \"test_example_protobuf.tfrecord\")\n",
    "\n",
    "with tf.io.TFRecordWriter(example_protobuf_path) as f:\n",
    "    f.write(serialized_person_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef0907-79f2-497d-aeba-cbc61639b6cc",
   "metadata": {},
   "source": [
    "## Load and Parse Example Protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "69bf0017-e67b-421e-8898-c459a9007dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'\\n[\\n\\x0b\\n\\x02id\\x12\\x05\\x1a\\x03\\n\\x01{\\n\\x11\\n\\x04name\\x12\\t\\n\\x07\\n\\x05Alice\\n9\\n\\x06emails\\x12/\\n-\\n\\x12alice123@gmail.com\\n\\x17alicecoolkid@icloud.com', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "serialized_examples_dataset = tf.data.TFRecordDataset(example_protobuf_path)\n",
    "\n",
    "for serialized_example in serialized_examples_dataset:\n",
    "    print(serialized_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001904d2-e271-4266-97fc-edf8ac1daecc",
   "metadata": {},
   "source": [
    "To parse this, we need to create a description dictionary for the features in the protobuf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "857fbd2c-83f8-41f4-bfd6-3911c232a0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'emails': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7f9c88ecbfd0>,\n",
       " 'id': <tf.Tensor: shape=(), dtype=int64, numpy=123>,\n",
       " 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Alice'>}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_description = {\n",
    "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    \"emails\": tf.io.VarLenFeature(tf.string),\n",
    "}\n",
    "\n",
    "for serialized_example in serialized_examples_dataset:\n",
    "    parsed_example = tf.io.parse_single_example(serialized_example, \n",
    "                                                feature_description)\n",
    "\n",
    "print(type(parsed_example))\n",
    "parsed_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "966f5d7e-507a-49bb-b0ea-36a65ba3c54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'alice123@gmail.com', b'alicecoolkid@icloud.com'], dtype=object)>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_example[\"emails\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1894d3c-f66c-4c23-a758-92bc050f712d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fashion MNIST using Data API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ecc882-ea97-4253-b7f9-beb445249d05",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628a7636-5189-439a-9186-3a60e16aa213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b347d4fd-7ad9-4ec6-af79-62a7e4c895e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58e79dd6-c7b4-47a0-9fba-1cb147d5c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "valid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df20e166-b6ba-430c-86f5-dfbb2ae6b6f9",
   "metadata": {},
   "source": [
    "Visualize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ee11283-09de-4745-bf80-54ad6475bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759880c5-678a-42e7-b02c-44b2e79c17d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## (Experimental) Save to tfrecord files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c8eddc-8936-4439-b5df-cd7ea2121871",
   "metadata": {},
   "source": [
    "We need to first convert the numpy arrays to Example protobufs, then we can serialize the protobufs and write to tfrecord files.\n",
    "- Convert numpy array to `Example` protobuf and serialize\n",
    "    - Automate this with function\n",
    "    - Apply to all feature data\n",
    "- Write protobufs to tfrecord files\n",
    "    - Each dataset (train, val, test) should be split into 20 files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a61e94-26e0-4973-8660-dfd9d502006c",
   "metadata": {},
   "source": [
    "### Convert Numpy Array to Protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a8983e-8d74-46c3-93b8-f1ec9c8e8b90",
   "metadata": {},
   "source": [
    "The Example protobuf should have 2 features:\n",
    "- serialized image from the numpy array (BytesList)\n",
    "- label of image (BytesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4154eb3e-6f89-4717-aeff-c9f47e2f00a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1b42ea2c-9231-4e9b-a0df-a7fb778a3a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_image = b\"bytestring of a serialized coat image matrix\"\n",
    "image_label = 4\n",
    "\n",
    "fashion_image_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            \"image\": Feature(\n",
    "                bytes_list=BytesList(\n",
    "                    value=[serialized_image]\n",
    "                )\n",
    "            ),\n",
    "            \"label\": Feature(\n",
    "                int64_list=Int64List(\n",
    "                    value=[image_label]\n",
    "                )\n",
    "            )\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "24e45a6b-4aaf-457a-951d-19170234866b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'bytestring of a serialized coat image matrix'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_image_example.features.feature[\"image\"].bytes_list.value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f59ec703-91af-44ad-aa61-ad519da41262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_coat = X_train[0]\n",
    "tensor_coat = tf.convert_to_tensor(numpy_coat)\n",
    "\n",
    "serialized_numpy_coat = tf.io.serialize_tensor(numpy_coat)\n",
    "serialized_tensor_coat = tf.io.serialize_tensor(tensor_coat)\n",
    "\n",
    "(serialized_numpy_coat == serialized_tensor_coat).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90231688-7aa5-45bd-b027-0c228bd4ea12",
   "metadata": {},
   "source": [
    "Numpy and tensors are often interchangeable, so we do not need to explicitly convert the numpy arrays into tensors to apply `tf.io.serialize_tensor()` in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a3ddd898-a5f1-480a-977b-bc1c0c49de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_to_protobuf(X, y):\n",
    "    X_protobufs = []\n",
    "    \n",
    "    for ix in range(X.shape[0]):\n",
    "        image = X[ix]\n",
    "        serialized_image = tf.io.serialize_tensor(image).numpy()\n",
    "        label = y[ix]\n",
    "        \n",
    "        image_protobuf = Example(\n",
    "            features=Features(\n",
    "                feature={\n",
    "                    \"serialized_image\": Feature(\n",
    "                        bytes_list=BytesList(\n",
    "                            value=[serialized_image]\n",
    "                        )\n",
    "                    ),\n",
    "                    \"label\": Feature(\n",
    "                        int64_list=Int64List(\n",
    "                            value=[label]\n",
    "                        )\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        X_protobufs.append(image_protobuf)\n",
    "        \n",
    "    return X_protobufs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "24e1b707-c1c9-4ada-a28f-0f87bf7970be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_protobufs = convert_image_to_protobuf(X_train, y_train)\n",
    "\n",
    "len(train_protobufs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad868ef7-543e-4985-87af-035e92588f37",
   "metadata": {},
   "source": [
    "### Parse Serialized Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "28b51fee-ed54-4e1d-9e0a-906fc2d5d0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x08\\x04\\x12\\x08\\x12\\x02\\x08\\x1c\\x12\\x02\\x08\\x1c\"\\x90\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00,\\x7f\\xb6\\xb9\\xa1x7\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00*\\xc6\\xfb\\xff\\xfb\\xf9\\xf7\\xff\\xfc\\xd6d\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\xe9\\xfc\\xed\\xef\\xea\\xed\\xeb\\xed\\xed\\xfe\\xe3\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x10\\xd2\\xe1\\xd7\\xaf\\xd9\\xd8\\xc1\\xc4\\xe2\\xdd\\xd12\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\xc7\\xe5\\xe8\\xe6\\xf5\\xcc\\xdb\\xfd\\xf5\\xcf\\xc2\\xdf\\xe7\\xec\\xeb\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x89\\xeb\\xcc\\xd1\\xc9\\xd1\\xea\\xbe\\xea\\xda\\xd7\\xee\\xef\\xcc\\xbd\\xe0\\x9a\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xc2\\xc9\\xc8\\xd1\\xca\\xc1\\xcd\\xc2\\xb7\\xda\\xe7\\xc5\\xac\\xb5\\xc1\\xcd\\xc7\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\xd4\\xcb\\xbc\\xbd\\xc4\\xc6\\xc6\\xc9\\xc4\\xd9\\xb3\\xa7\\xb7\\xd9\\xc5\\xca\\xdb\\x1e\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\\xe1\\xc8\\xc2\\xbe\\xbc\\xc0\\xc4\\xc0\\xaa\\xca\\xbe\\xc9\\xc3\\xc8\\xc9\\xd1\\xe32\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00D\\xe1\\xd2\\xd3\\xc6\\xc0\\xc4\\xcc\\xc4\\xb5\\xd4\\xc5\\xc3\\xc0\\xce\\xdc\\xd2\\xe5]\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00o\\xdf\\xe3\\xfd\\xd1\\xc4\\xcc\\xd3\\xce\\xb7\\xd8\\xce\\xd2\\xcb\\xd7\\xf4\\xe0\\xe3\\x96\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8b\\xe1\\xe0\\xff\\xca\\xce\\xd4\\xd1\\xd3\\xbe\\xd5\\xca\\xcf\\xce\\xde\\xff\\xe6\\xdc\\xbe\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xb4\\xe2\\xe0\\xff\\xc7\\xcc\\xcf\\xd6\\xd6\\xbe\\xd8\\xce\\xcb\\xcd\\xdb\\xf3\\xe0\\xd6\\xea\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xe1\\xdf\\xe4\\xfe\\xd1\\xce\\xd0\\xd5\\xd2\\xbf\\xd7\\xcf\\xcc\\xd0\\xd3\\xf9\\xe2\\xd6\\xff&\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xfa\\xe8\\xf0\\xef\\xd3\\xcb\\xd1\\xcd\\xd3\\xc5\\xd7\\xd0\\xd0\\xd6\\xd5\\xef\\xe7\\xdb\\xffQ\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf8\\xec\\xf7\\xf0\\xcb\\xc8\\xd0\\xce\\xd6\\xc1\\xd5\\xd4\\xd0\\xd4\\xd3\\xf3\\xf2\\xe1\\xfeB\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf7\\xe6\\xfc\\xe2\\xc7\\xd3\\xca\\xd3\\xd5\\xb6\\xd5\\xd4\\xce\\xca\\xdb\\xcf\\xf7\\xde\\xedh\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\xf4\\xdb\\xfa\\xcd\\xc7\\xd1\\xca\\xd1\\xd3\\xbd\\xd6\\xce\\xd2\\xc8\\xd4\\x9a\\xf0\\xd0\\xdb\\x8c\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x15\\xff\\xde\\xee\\xb8\\xd2\\xc0\\xce\\xd1\\xd2\\xbd\\xd5\\xd3\\xd1\\xc0\\xe4\\x9b\\xe2\\xee\\xf1\\xa6\\x00\\x00\\x00\\x00\\x00\\x00\\x00%\\xf5\\xe2\\xf1\\x96\\xc5\\xbd\\xcc\\xd1\\xd2\\xb7\\xd5\\xd5\\xc9\\xb8\\xd7\\x92\\xd8\\xec\\xe1\\x9a\\x00\\x00\\x00\\x00\\x00\\x00\\x00:\\xef\\xe3\\xff\\x9e\\xc1\\xc3\\xcc\\xd1\\xd5\\xb4\\xcf\\xd9\\xc7\\xc2\\xd3\\x9e\\xdb\\xec\\xd8\\x97\\x00\\x00\\x00\\x00\\x00\\x00\\x00D\\xe9\\xe2\\xf3\\x8b\\xc8\\xc1\\xcd\\xd2\\xd0\\xb4\\xcd\\xd4\\xcb\\xc4\\xd8\\x9d\\xb3\\xff\\xd8\\x9b\\x00\\x00\\x00\\x00\\x00\\x00\\x00Q\\xe1\\xe0\\xd3\\x8a\\xdb\\xb9\\xc9\\xd5\\xcf\\xc5\\xe2\\xd4\\xc8\\xbe\\xd7\\xb7Z\\xff\\xd3\\x93\\x00\\x00\\x00\\x00\\x00\\x00\\x00[\\xd2\\xe6\\x9er\\xcd\\xbb\\xd0\\xd1\\xce\\xc1\\xd2\\xd3\\xcc\\xc3\\xcc\\xb5\\x17\\xff\\xd5\\x9e\\x00\\x00\\x00\\x00\\x00\\x00\\x00W\\xcd\\xe8m\\xa4\\xff\\xd6\\xe0\\xde\\xd2\\xc5\\xd6\\xe1\\xde\\xd3\\xdc\\xd9\\x00\\xea\\xd8\\xa9\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\\\\\xd5\\xe8\\x92\\x05\\x86\\x97\\xa2\\xaa\\xb7\\xb6\\xa4\\xa6\\xb2\\xa2\\x9cb\\x00\\xf0\\xe1\\xd2\\x00\\x00\\x00\\x00\\x00\\x00\\x00+\\xa4\\xce\\x8d\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x7f}L\\x00\\x00\\x00'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serialized_coat = train_protobufs[0].features.feature[\"serialized_image\"].bytes_list.value[0]\n",
    "serialized_coat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a2876023-1f66-456e-872b-7025b78d07cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parsed_coat = tf.io.parse_tensor(serialized_coat, out_type=tf.uint8)\n",
    "plot_image(parsed_coat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "dc43e38d-56d6-43f4-b6ac-aa4cf8a6756d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_protobufs[0].features.feature[\"label\"].int64_list.value[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44410ac-37ab-4770-a11c-1689ad55b3c6",
   "metadata": {},
   "source": [
    "### Write to tfrecord files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2c4e63-4b0c-44ac-8cf5-ad8130f1a210",
   "metadata": {},
   "source": [
    "Let's convert the val and test sets too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "b71d3821-8231-45cc-b58a-9010bda111af",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_protobufs = convert_image_to_protobuf(X_val, y_val)\n",
    "test_protobufs = convert_image_to_protobuf(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1306ce95-9a20-402d-ba61-e1f1ae177c74",
   "metadata": {},
   "source": [
    "We need to do the following:\n",
    "- Get the indices of 20 equal sized lists\n",
    "    - Do this for each of the train, val, test sets\n",
    "- Create a list of filepaths for 20 tfrecord data files with suffix _00, _01, ... , _19, _20\n",
    "    - Do this for each of the train, val, test sets\n",
    "- Write the protobufs to each of the 20 files using the indices and filepaths\n",
    "    - Do this for each of the train, val, test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea7dd0b-ebc9-4da5-8020-abda49a12d9e",
   "metadata": {},
   "source": [
    "**Get indices of split lists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "8625e5e5-a554-4127-ad84-4741f4bd2407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_chunks = 20\n",
    "\n",
    "val_indices = np.random.permutation(len(X_val))\n",
    "val_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "bb9d1599-f770-4cfd-b4d1-5bf46c3e22ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_indices = np.split(val_indices, n_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a12d4a-8704-471d-a582-4a25d5c9b539",
   "metadata": {},
   "source": [
    ">**[numpy.split](https://numpy.org/doc/stable/reference/generated/numpy.split.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b808e59c-3367-42b7-9ec3-aa95c29036f3",
   "metadata": {},
   "source": [
    "**Get filepaths for each split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9ad08ce6-e540-44e9-ae27-8c4b08221dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "FASHION_DATA_DIR = os.path.join(DATA_DIR, \"my_fashion_mnist\")\n",
    "os.makedirs(FASHION_DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f05a0452-cc46-4994-a575-e35d18052d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./_datasets/12_tf_data/my_fashion_mnist/val_00.tfrecord',\n",
       " './_datasets/12_tf_data/my_fashion_mnist/val_01.tfrecord',\n",
       " './_datasets/12_tf_data/my_fashion_mnist/val_02.tfrecord',\n",
       " './_datasets/12_tf_data/my_fashion_mnist/val_03.tfrecord',\n",
       " './_datasets/12_tf_data/my_fashion_mnist/val_04.tfrecord',\n",
       " './_datasets/12_tf_data/my_fashion_mnist/val_05.tfrecord',\n",
       " './_datasets/12_tf_data/my_fashion_mnist/val_06.tfrecord',\n",
       " './_datasets/12_tf_data/my_fashion_mnist/val_07.tfrecord',\n",
       " './_datasets/12_tf_data/my_fashion_mnist/val_08.tfrecord',\n",
       " './_datasets/12_tf_data/my_fashion_mnist/val_09.tfrecord',\n",
       " './_datasets/12_tf_data/my_fashion_mnist/val_10.tfrecord',\n",
       " './_datasets/12_tf_data/my_fashion_mnist/val_11.tfrecord',\n",
       " './_datasets/12_tf_data/my_fashion_mnist/val_12.tfrecord',\n",
       " './_datasets/12_tf_data/my_fashion_mnist/val_13.tfrecord',\n",
       " './_datasets/12_tf_data/my_fashion_mnist/val_14.tfrecord',\n",
       " './_datasets/12_tf_data/my_fashion_mnist/val_15.tfrecord',\n",
       " './_datasets/12_tf_data/my_fashion_mnist/val_16.tfrecord',\n",
       " './_datasets/12_tf_data/my_fashion_mnist/val_17.tfrecord',\n",
       " './_datasets/12_tf_data/my_fashion_mnist/val_18.tfrecord',\n",
       " './_datasets/12_tf_data/my_fashion_mnist/val_19.tfrecord']"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_filepaths = [\"val_{:02d}.tfrecord\".format(index)\n",
    "                 for index in range(n_chunks)]\n",
    "\n",
    "val_filepaths = [os.path.join(FASHION_DATA_DIR, filepath)\n",
    "                 for filepath in val_filepaths]\n",
    "\n",
    "val_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bf4592-5d14-4301-991d-1c81a985053b",
   "metadata": {},
   "source": [
    "**Write protobufs to filepaths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "98ada3d2-2387-4ea3-bb19-33b6af15f1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3631,  811, 2824, 3313, 1343, 3545, 1832, 3252,  956,  978, 3224,\n",
       "       4997, 2302, 1809, 2065,  950,   22,  947, 4945, 4586, 3891, 3715,\n",
       "        309, 3413,  191, 2499, 4521, 4952,  499,  240, 1978, 4302,  274,\n",
       "       1970, 2312, 3170, 1856, 3122, 2148, 3871,  848, 2294,  942, 1339,\n",
       "       2568, 3820, 2842,  862, 4394, 4836,  545, 2834, 1513, 4298, 3212,\n",
       "       2800, 2264, 1430, 4947,  542, 3251, 3546, 4808,  197, 2070, 1044,\n",
       "       3346,  874, 4514, 3316,  803,  268, 2079, 4756, 4838,  130,  523,\n",
       "       2089, 3661, 1315,   87, 2816, 4954, 1224, 4630, 1719, 2967,  169,\n",
       "       1508, 3547, 2826, 4058, 3423, 1525,  453,  511, 2692, 4484, 1495,\n",
       "       2999, 2108, 2782,   59, 1192,  789, 4294, 2779, 1614, 4639, 3646,\n",
       "       2356, 2927, 4009, 2273,  172,  387, 2500,  368, 2112,  143, 2401,\n",
       "        914, 3913, 1890,  450, 3865, 3105, 4286, 2565, 1896, 4706, 2138,\n",
       "       4538,  263, 2137, 4901, 1750, 2581,  756, 3767, 4186, 4862, 4494,\n",
       "       3716,   54, 3695,  960,  854,  123, 2704, 3656, 1973,  446, 3088,\n",
       "       4492, 3604, 2150, 2923, 4798, 3174, 2102, 2410, 2133, 1692, 3801,\n",
       "       4195, 1225, 1740, 3578, 1384, 1473, 3657, 1173, 4372, 4522, 4613,\n",
       "       3916, 3007, 4069, 1161, 3380,  520, 1548, 1582, 3368, 4117,  961,\n",
       "       1591, 4262, 1100, 3595,  155,   39, 3726,  850, 3355, 3172, 2665,\n",
       "       2001, 4989,   48, 3153, 2749, 2511, 4527, 1344, 4815, 1643, 1531,\n",
       "       4122, 2758, 1283, 1736, 4038, 4749, 4751, 3286, 4310,  175,  246,\n",
       "       1320, 3272, 4511, 4865, 2098, 3822, 2379, 3466, 2942, 4095, 2552,\n",
       "       3705,  423, 3372, 3190, 1269, 2976,  841, 3575,  864, 3861, 4775,\n",
       "       2696, 3616, 2896, 3768, 4799,  686, 1356, 4004])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "a12864f3-c347-4efc-8464-21c1b23be44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in range(n_chunks):\n",
    "    val_ix = val_indices[chunk]\n",
    "    val_fp = val_filepaths[chunk]\n",
    "    \n",
    "    with tf.io.TFRecordWriter(val_fp) as f:\n",
    "        for ix in val_ix:\n",
    "            f.write(val_protobufs[ix].SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eff017-d44c-4a67-a9d5-c57f70acda62",
   "metadata": {},
   "source": [
    "### Load tfrecord as dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7db2ff-fbee-4214-816d-0b93090c122f",
   "metadata": {},
   "source": [
    "Load one of the split files and parse string to protobuf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "5a16d284-3263-45eb-a61b-c93935da5750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_description = {\n",
    "    \"serialized_image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "    \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "}\n",
    "\n",
    "serialized_val = tf.data.TFRecordDataset(val_filepaths[0])\n",
    "\n",
    "for val_protobuf in serialized_val.take(1):\n",
    "    parsed_val = tf.io.parse_single_example(val_protobuf, \n",
    "                                            feature_description)\n",
    "    \n",
    "type(parsed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "7b5e1365-2d6d-4236-8b9b-05d5bb8ef5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': <tf.Tensor: shape=(), dtype=int64, numpy=9>,\n",
       " 'serialized_image': <tf.Tensor: shape=(), dtype=string, numpy=b'\\x08\\x04\\x12\\x08\\x12\\x02\\x08\\x1c\\x12\\x02\\x08\\x1c\"\\x90\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x0094X3@\\xaf\\x8dgK93\\'\\'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x00r\\xcb\\xd1\\xde{\\x90\\xd0\\xcb\\xd7\\xdc\\xe5\\xbe\\xf30\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x1d\\x9d\\xcb\\xff\\x9b\\xc2\\xd9\\xda\\xd5\\xd4\\xcb\\xce\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x01\\x00\\x18n\\x87\\xc6\\xc5\\xc5\\xb3\\xbc\\xbd\\xbe\\xbe\\xb8`V\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x01\\x00p\\xd7\\xab\\xaf\\xbc\\xd5\\xd7\\xb5\\xb8\\xba\\xc3\\xca\\xf4\\'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x06\\x8e\\xdd\\xf9\\xd3\\xc2\\xb5\\xd8\\xd7\\xe1\\xcf\\xcc\\xd5!\\x00\\x00\\x00\\x00\\x00\\x03\\x00\\x01\\x00\\x00\\x03\\x00\\x00\\x0c\\x94\\xdduZ\\xd1\\xbf\\xba\\xbe\\xc5\\xc3\\xbe\\xc3\\xd3)\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x02\\x00\\x00\\x00?\\xab\\xa3\\xc7\\x92\\x8b\\xbf\\xb5\\xb9\\xcb\\xcf\\xda\\xd3\\xc2\\xce4\\x00\\x00\\x01\\x00\\x01\\x00\\x01\\x02\\x00\\x00\\x10\\x99\\x9b\\xce\\x9a4\\x9a\\xa6\\xb5\\xb8\\xc2\\xcb\\xc8\\xc3\\xcf\\xba\\xc6B\\x00\\x00\\x02\\x01\\x00\\x02\\x03\\x00\\x00l\\x94\\xcb\\xb4\\x80\\x9bw\\xba\\xc6\\xc8\\xc3\\xc8\\xc3\\xc2\\xc3\\xba\\x9e\\xbdm\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x9e\\xd4\\x80\\x94{0\\xca\\xc8\\xcb\\xc6\\xbe\\xc3\\xc5\\xc2\\xc7\\xb8\\xb5\\xac\\xb8\\x90\\x00\\x00\\x19WZ\\\\d\\x82<\\xa0`i\\xd7\\xcb\\xc5\\xb9\\xbc\\xbc\\xbe\\xc3\\xc3\\xbe\\xb0\\xa6\\xaa\\xac\\xba\\xa0\\x00\\x00w\\xab\\xa4\\x97\\x99\\x9d\\x90\\x9b\\xa7\\xa8\\x9f\\x9f\\xa2\\xab\\xb6\\xc5\\xcc\\xce\\xce\\xcb\\xd8\\xce\\xd1\\xb5\\xc3\\xa8\\x00\\x01\\x01\\x15X\\x80\\x8b\\x9f\\xad\\xba\\xbf\\xba\\xbe\\xde\\xe5\\xe0\\xcb\\xb6\\xa8\\x9d\\x94\\x88{cX@<%\\x00\\x122\\x10\\x01\\x00\\x01\\x00\\x05\\x1d$\\'$\\x19\\x0b\\x03\\x01\\x01\\x02\\x02\\x03\\x03\\x03\\x06\\x0e\\x13\\x194\\x13\\x064?=4) \\x17\\x0f\\x10\\x12\\x14\\x13\\x1e$-0479;;<<;;A\\x05\\x00\\x00\\x12+879<=;;;9<;99744332799<\\t\\x00\\x00\\x00\\x00\\x03\\n\\x13\\x1c!$$$$!!\"$%\\'\\'\\'\\'*)!\\x1d\\x0c\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'>}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe24eca-212e-45a4-84ce-8f6d660b3028",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save to tfrecord files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd5b7ce-df5f-4871-9378-7902ccbcc1a4",
   "metadata": {},
   "source": [
    "- Convert numpy arrays to Example protobufs\n",
    "- Get indices and filepaths for n chunks\n",
    "- Store each chunk as a tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e9fb19-130e-4150-91de-0d96b02a8822",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4710a777-ef4a-4719-a1e9-88e40d280b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_to_example(image, label):\n",
    "    image_data = tf.io.serialize_tensor(image)\n",
    "\n",
    "    return Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "                \"image\": Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n",
    "                \"label\": Feature(int64_list=Int64List(value=[label])),\n",
    "            }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d45eca2-2778-445b-966b-ba1bcb9cc850",
   "metadata": {
    "tags": []
   },
   "source": [
    "### save_as_tf_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b989bb9-47f8-4548-acc8-2c7fc8056972",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_SAVE_DIR = os.path.join(\".\", \"_datasets\", \"12_tf_data\", \"my_fashion_mnist\")\n",
    "os.makedirs(FILE_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "def write_tfrecords(name, dataset, n_chunks=10):\n",
    "    paths = [os.path.join(FILE_SAVE_DIR, \n",
    "                          \"{}.tfrecord-{:05d}-of-{:05d}\".format(name, index, n_chunks))\n",
    "             for index in range(n_chunks)]\n",
    "    writers = [tf.io.TFRecordWriter(path)\n",
    "               for path in paths]\n",
    "    \n",
    "    for index, (image, label) in dataset.enumerate():\n",
    "        chunk = index % n_chunks\n",
    "        example = convert_image_to_example(image, label)\n",
    "        writers[chunk].write(example.SerializeToString())\n",
    "    \n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac61c6e-7f2a-49fc-bfcc-4cd0adcc9208",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba8e0ede-b62f-4262-8b57-b8f8c7841206",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepaths = write_tfrecords(\"my_fashion_mnist.train\", train_set)\n",
    "valid_filepaths = write_tfrecords(\"my_fashion_mnist.valid\", valid_set)\n",
    "test_filepaths = write_tfrecords(\"my_fashion_mnist.test\", test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e7934b-f5d4-47ea-9e4f-4f1ac57d8413",
   "metadata": {},
   "source": [
    "## Load tfrecords as datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e49fd28-73b6-4a5c-ba52-2e7d5105e2f5",
   "metadata": {},
   "source": [
    "Now that we have 20 tfrecords for train, val, and test sets, each file containing serialized image protobufs, let's load and interleave the files into dataset objects.\n",
    "<br>\n",
    "We need to do the following:\n",
    "- Get all files corresponding to each set (train, val, test)\n",
    "- Interleave the lines in each file\n",
    "- Shuffle the dataset\n",
    "- Preprocess each serialized protobuf\n",
    "    - Create a feature description\n",
    "    - Parse the serialized protobuf\n",
    "    - Return a tuple containing a tensor of the image array and the label\n",
    "        - Parse the serialized image tensor\n",
    "- Batch the dataset\n",
    "- Prefetch the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b675a968-eb9e-49ea-8470-1670eaa77ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- NOT DEBUGGING ----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "if DEBUG:\n",
    "    print(\"\\n---------- DEBUGGING ----------\\n\")\n",
    "    \n",
    "    set_name = \"val\"\n",
    "    \n",
    "    # get filenames\n",
    "    filenames = os.path.join(FILE_SAVE_DIR, set_name + \"_*\")\n",
    "    filenames_dataset = tf.data.Dataset.list_files(filenames, shuffle=True, seed=SEED)\n",
    "    print(\"\\n --- Items in filenames_dataset --- \\n\")\n",
    "    for item in filenames_dataset.take(3):\n",
    "        print(item)\n",
    "    \n",
    "    # test to see if TFRecordDataset automatically interleaves when given num_parallel_reads > 1\n",
    "    for filename in filenames_dataset.take(1):\n",
    "        dataset = tf.data.TFRecordDataset(filename, num_parallel_reads=1)\n",
    "    print(\"\\n --- Items in dataset (non-interleaved) --- \\n\")\n",
    "    for item in dataset.take(1):\n",
    "        print(item)\n",
    "        \n",
    "    # parse serialized protobuf\n",
    "    feature_description = {\n",
    "        \"serialized_image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1)\n",
    "    }\n",
    "    \n",
    "    print(\"\\n --- Parsed item in dataset (non-interleaved) --- \\n\")\n",
    "    for item in dataset.take(1):\n",
    "        parsed_item = tf.io.parse_single_example(item, feature_description)\n",
    "        print(type(parsed_item), \"\\n\", parsed_item)\n",
    "    \n",
    "    # parse tensor image\n",
    "    print(\"\\n --- Parsed item with parsed image tensor in dataset (non-interleaved) --- \\n\")\n",
    "    parsed_item[\"serialized_image\"] = tf.io.parse_tensor(parsed_item[\"serialized_image\"],\n",
    "                                                         out_type=tf.uint8)\n",
    "    print(type(parsed_item), \"\\n\", parsed_item)\n",
    "    \n",
    "    # method that does all the parsing\n",
    "    def parse_image_protobuf(serialized_protobuf):\n",
    "        feature_description = {\n",
    "            \"serialized_image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "            \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1)\n",
    "        }\n",
    "        parsed_dict = tf.io.parse_single_example(serialized_protobuf, feature_description)\n",
    "        parsed_dict[\"serialized_image\"] = tf.io.parse_tensor(parsed_dict[\"serialized_image\"],\n",
    "                                                             out_type=tf.uint8)\n",
    "        return parsed_dict[\"serialized_image\"], tf.cast(parsed_dict[\"label\"], tf.uint8)\n",
    "        \n",
    "    # parsed tfrecord without interleave\n",
    "    for filename in filenames_dataset.take(1):\n",
    "        dataset = tf.data.TFRecordDataset(filename, num_parallel_reads=1)\n",
    "    print(\"\\n --- Parsed item in dataset (non-interleaved) --- \\n\")\n",
    "    for item in dataset.take(1):\n",
    "        image, label = parse_image_protobuf(item)\n",
    "        print(\"Label: \", label)\n",
    "        print(\"Image shape: \", image.shape)\n",
    "        plot_image(image)\n",
    "        \n",
    "    # parsed tfrecord with interleave\n",
    "    dataset = tf.data.TFRecordDataset(filenames_dataset, num_parallel_reads=AUTOTUNE)\n",
    "    print(\"\\n --- Parsed item in dataset (interleaved) --- \\n\")\n",
    "    for item in dataset.take(1):\n",
    "        image, label = parse_image_protobuf(item)\n",
    "        print(\"Label: \", label)\n",
    "        plot_image(image)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n---------- NOT DEBUGGING ----------\\n\")\n",
    "    \n",
    "    # method that does all the parsing\n",
    "    def preprocess(tfrecord):\n",
    "        feature_descriptions = {\n",
    "            \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "            \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1)\n",
    "        }\n",
    "        example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n",
    "        image = tf.io.parse_tensor(example[\"image\"], out_type=tf.uint8)\n",
    "        image = tf.reshape(image, shape=[28, 28])\n",
    "        \n",
    "        return image, example[\"label\"]\n",
    "    \n",
    "    # method that returns optimized dataset object\n",
    "    def fashion_mnist_dataset(filenames,\n",
    "                              cache=True,\n",
    "                              n_read_threads=AUTOTUNE, \n",
    "                              n_parse_threads=AUTOTUNE,\n",
    "                              shuffle_buffer_size=None, \n",
    "                              batch_size=32):\n",
    "        \"\"\" \n",
    "        Takes a set_name like \"train\" and creates and returns an optimized dataset \n",
    "        object that loads from tfrecord files in interleaved order, shuffles the dataset, \n",
    "        parses the serialized protobufs in the dataset, batches the dataset, \n",
    "        and prefetches the dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # load serialized protobufs\n",
    "        dataset = tf.data.TFRecordDataset(filenames, \n",
    "                                          num_parallel_reads=n_read_threads)\n",
    "        \n",
    "        # shuffle and repeat dataset\n",
    "        if cache:\n",
    "            dataset = dataset.cache()\n",
    "        if shuffle_buffer_size:\n",
    "            dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "        \n",
    "        # parse dataset\n",
    "        dataset = dataset.map(preprocess, \n",
    "                              num_parallel_calls=n_parse_threads)\n",
    "        \n",
    "        # batch and prefetch\n",
    "        return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05e84c8b-e537-44b2-844a-df11570f3a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = fashion_mnist_dataset(train_filepaths, shuffle_buffer_size=60000)\n",
    "valid_set = fashion_mnist_dataset(valid_filepaths, shuffle_buffer_size=5000)\n",
    "test_set = fashion_mnist_dataset(test_filepaths, shuffle_buffer_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82216897-e621-40ba-b2eb-e1a47caa8891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- NOT DEBUGGING ----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "if DEBUG:\n",
    "    print(\"\\n---------- DEBUGGING ----------\\n\")\n",
    "    \n",
    "    sample_dataset = test_dataset\n",
    "    sample_set = X_test\n",
    "    \n",
    "    for batch in sample_dataset.take(1):\n",
    "        sample_batch = batch\n",
    "    \n",
    "    # Batch is a tuple containing (X, y)\n",
    "    # where X and y are values for 32 individual samples\n",
    "    print(\"\\n--- First image input in dataset ---\\n\")\n",
    "    print(attribute_names[sample_batch[1][0].numpy()])\n",
    "    plot_image(sample_batch[0][0])\n",
    "    \n",
    "    # Make sure the efficient dataset contains all items\n",
    "    print(\"\\n--- Compare number of batches (should be equal) ---\\n\")\n",
    "    batch_size = 32\n",
    "    print(len(sample_set) / batch_size)\n",
    "    i = 0\n",
    "    for batch in sample_dataset:\n",
    "        i += 1\n",
    "    print(i)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n---------- NOT DEBUGGING ----------\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a86848-f2f6-49a5-bd02-1ea5deb33fb1",
   "metadata": {},
   "source": [
    "## Train model on datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce98a8e4-0a0a-406b-9480-f3d227e6563e",
   "metadata": {},
   "source": [
    "So now that we have efficient dataset objects that will load, interleave, shuffle, parse, batch, and prefetch the data, let's train our model.\n",
    "<br>\n",
    "\n",
    "We need to do the following:\n",
    "- Create a preprocessing layer that standardizes inputs\n",
    "- Create our sequential classification model\n",
    "- Find optimal learning rate\n",
    "- Create callbacks\n",
    "- Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d87a7f-1658-4e0e-a53a-57c53267bc6d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a4ac5b5-b198-4b5b-a2a1-18380ffd72be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- NOT DEBUGGING ----------\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " normalization (Normalizatio  (None, 784)              1569      \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 300)               90300     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 300)               90300     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                3010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 420,679\n",
      "Trainable params: 419,110\n",
      "Non-trainable params: 1,569\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "if DEBUG:\n",
    "    print(\"\\n---------- DEBUGGING ----------\\n\")\n",
    "    \n",
    "    \n",
    "    # Adapt and Input data processing\n",
    "    \n",
    "    n_batches = 10\n",
    "    batch_size = 32\n",
    "    adapt_shape = [n_batches * batch_size, 28, 28]\n",
    "    adapt_dataset = train_dataset\n",
    "    \n",
    "    adapt_data = tf.stack([batch[0] for batch in adapt_dataset.take(n_batches)])\n",
    "    adapt_data = tf.reshape(adapt_data, adapt_shape)\n",
    "    print(\"\\n--- Adapt data shape ---\\n\")\n",
    "    print(adapt_data.shape)\n",
    "    \n",
    "    input_data = tf.stack([batch[0][0] for batch in val_dataset.take(1)])\n",
    "    print(\"\\n--- Input data shape ---\\n\")\n",
    "    print(input_data.shape)\n",
    "    \n",
    "    \n",
    "    # Normalization Layer\n",
    "    \n",
    "    std_layer = keras.layers.Normalization(axis=(-2,-1))\n",
    "    std_layer.adapt(adapt_data)\n",
    "\n",
    "    scaled_image = std_layer(input_data)\n",
    "    print(\"\\n--- Scaled input data ---\\n\")\n",
    "    print(scaled_image.shape)\n",
    "    plot_image(scaled_image[0])\n",
    "    \n",
    "    \n",
    "    # Flatten Layer\n",
    "    \n",
    "    input_layer = keras.layers.Flatten(input_shape=[28, 28])\n",
    "    \n",
    "    \n",
    "    # Unbatch adapt and input data\n",
    "    \n",
    "    n_samples = 1000\n",
    "    adapt_dataset = train_dataset\n",
    "    \n",
    "    adapt_data = tf.stack([sample[0] for sample in adapt_dataset.unbatch().take(n_samples)])\n",
    "    print(\"\\n--- Unbatched Adapt data shape ---\\n\")\n",
    "    print(adapt_data.shape)\n",
    "    \n",
    "    input_data = tf.stack([sample[0] for sample in val_dataset.unbatch().take(1)])\n",
    "    print(\"\\n--- Unbatched Input data shape ---\\n\")\n",
    "    print(input_data.shape)\n",
    "    \n",
    "    \n",
    "    # Flatten adapt and input data\n",
    "    \n",
    "    flattened_adapt_data = input_layer(adapt_data)\n",
    "    print(\"\\n--- Flattened Adapt data shape ---\\n\")\n",
    "    print(flattened_adapt_data.shape)\n",
    "\n",
    "    flattened_input_data = input_layer(input_data)\n",
    "    print(\"\\n--- Flattened Input data shape ---\\n\")\n",
    "    print(flattened_input_data.shape)\n",
    "    \n",
    "    \n",
    "    # Normalization layer on flattened data\n",
    "    \n",
    "    std_layer = keras.layers.Normalization(axis=-1)\n",
    "    std_layer.adapt(flattened_adapt_data)\n",
    "    scaled_input = std_layer(flattened_input_data)\n",
    "    \n",
    "    print(\"\\n--- Scaled Flattened Input data shape ---\\n\")\n",
    "    print(scaled_input.shape)\n",
    "    plot_image(tf.reshape(scaled_input, [28, 28]))\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"\\n---------- NOT DEBUGGING ----------\\n\")\n",
    "    \n",
    "    \n",
    "    def create_fashion_model(adapt_dataset, \n",
    "                             n_hidden=3, n_neurons=300, \n",
    "                             n_adapt_samples=1000):\n",
    "        reset_backend()\n",
    "\n",
    "        # Create model\n",
    "        model = keras.Sequential()\n",
    "\n",
    "        # Flatten Layer\n",
    "        input_layer = keras.layers.Flatten(input_shape=[28, 28])\n",
    "        model.add(input_layer)\n",
    "\n",
    "        # Normalization Layer\n",
    "        adapt_data = tf.stack([sample[0] for sample in adapt_dataset.unbatch().take(n_adapt_samples)])\n",
    "        flattened_adapt_data = input_layer(adapt_data)\n",
    "\n",
    "        std_layer = keras.layers.Normalization(axis=-1)\n",
    "        std_layer.adapt(flattened_adapt_data)\n",
    "        model.add(std_layer)\n",
    "\n",
    "        # Hidden Layers\n",
    "        for _ in range(n_hidden):\n",
    "            model.add(keras.layers.Dense(n_neurons, \n",
    "                                         activation=\"elu\", \n",
    "                                         kernel_initializer=\"he_normal\"))\n",
    "\n",
    "        # Output Layer\n",
    "        model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "        return model\n",
    "    \n",
    "    model = create_fashion_model(train_set)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "571d4326-d477-48ae-aa76-9343100c6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=1e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f240103-711e-48b1-82e8-c457d8090814",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### LR Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "ee6bddc9-1c27-4041-9a0b-238141a0bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up_to_tenth(n):\n",
    "    a = np.log(n) / np.log(10)\n",
    "    a_ = np.rint(a)\n",
    "    return 10**a_\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-10, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
    "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses, max_loss=None):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    \n",
    "    tick = round_up_to_tenth(min(rates))\n",
    "    x_ticks = []\n",
    "    while tick < max(rates)*10:\n",
    "        x_ticks.append(tick)\n",
    "        tick = tick * 10\n",
    "    plt.gca().set_xticks(x_ticks)\n",
    "    \n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    if max_loss is None:\n",
    "        plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    else:\n",
    "        plt.axis([min(rates), max(rates), min(losses), max_loss])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904e2216-6d1c-4d2a-9bff-458b71d413e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Find Optimal LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "e203b0a4-6bab-4083-aefa-05b1cf3c49c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430/430 [==============================] - 4s 6ms/step - loss: 1183.1133 - accuracy: 0.2639\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train, y_train,\n",
    "                                   epochs=epochs, batch_size=batch_size,\n",
    "                                   min_rate=1e-8, max_rate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "01972eb3-eef4-4ccd-a433-749a76919eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plot_lr_vs_loss(rates, losses, max_loss=3.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be50fbc-511e-4024-aeb5-e84e19533296",
   "metadata": {},
   "source": [
    "Let's go with `5e-4`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58368dcf-849e-4e88-ac85-46a6657556d2",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2492658-2964-4aaa-91bb-39d1d2861e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- TRAINING ----------\n",
      "\n",
      "\n",
      "--- TensorBoard Directory: ./_tf_logs/12_tf_data/first_model_run_03 ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 11:50:13.912999: I tensorflow/core/profiler/lib/profiler_session.cc:99] Profiler session initializing.\n",
      "2022-07-04 11:50:13.913028: I tensorflow/core/profiler/lib/profiler_session.cc:114] Profiler session started.\n",
      "2022-07-04 11:50:13.913220: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session tear down.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "     39/Unknown - 1s 6ms/step - loss: 7189.3735 - accuracy: 0.6731"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 11:50:15.216706: I tensorflow/core/profiler/lib/profiler_session.cc:99] Profiler session initializing.\n",
      "2022-07-04 11:50:15.216723: I tensorflow/core/profiler/lib/profiler_session.cc:114] Profiler session started.\n",
      "2022-07-04 11:50:15.222550: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-07-04 11:50:15.225455: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session tear down.\n",
      "2022-07-04 11:50:15.230433: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./_tf_logs/12_tf_data/first_model_run_03/plugins/profile/2022_07_04_11_50_15\n",
      "\n",
      "2022-07-04 11:50:15.232827: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./_tf_logs/12_tf_data/first_model_run_03/plugins/profile/2022_07_04_11_50_15/evilonna-C.local.trace.json.gz\n",
      "2022-07-04 11:50:15.238399: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./_tf_logs/12_tf_data/first_model_run_03/plugins/profile/2022_07_04_11_50_15\n",
      "\n",
      "2022-07-04 11:50:15.238686: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./_tf_logs/12_tf_data/first_model_run_03/plugins/profile/2022_07_04_11_50_15/evilonna-C.local.memory_profile.json.gz\n",
      "2022-07-04 11:50:15.240909: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./_tf_logs/12_tf_data/first_model_run_03/plugins/profile/2022_07_04_11_50_15\n",
      "Dumped tool data for xplane.pb to ./_tf_logs/12_tf_data/first_model_run_03/plugins/profile/2022_07_04_11_50_15/evilonna-C.local.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./_tf_logs/12_tf_data/first_model_run_03/plugins/profile/2022_07_04_11_50_15/evilonna-C.local.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./_tf_logs/12_tf_data/first_model_run_03/plugins/profile/2022_07_04_11_50_15/evilonna-C.local.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./_tf_logs/12_tf_data/first_model_run_03/plugins/profile/2022_07_04_11_50_15/evilonna-C.local.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./_tf_logs/12_tf_data/first_model_run_03/plugins/profile/2022_07_04_11_50_15/evilonna-C.local.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 11s 6ms/step - loss: 446.2132 - accuracy: 0.8398 - val_loss: 2039.4036 - val_accuracy: 0.8734\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 831.1343 - accuracy: 0.8842 - val_loss: 249.6432 - val_accuracy: 0.8812\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 913.2344 - accuracy: 0.9004 - val_loss: 91.7301 - val_accuracy: 0.8806\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 51.7051 - accuracy: 0.9121 - val_loss: 589.5067 - val_accuracy: 0.8900\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 62.5098 - accuracy: 0.9219 - val_loss: 377.7085 - val_accuracy: 0.8932\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 174.0726 - accuracy: 0.9305 - val_loss: 242.0870 - val_accuracy: 0.8908\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 140.6279 - accuracy: 0.9393 - val_loss: 52.8440 - val_accuracy: 0.8858\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 35.5376 - accuracy: 0.9441 - val_loss: 321.4366 - val_accuracy: 0.8962\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 98.8371 - accuracy: 0.9514 - val_loss: 8.6645 - val_accuracy: 0.8910\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 18.1179 - accuracy: 0.9560 - val_loss: 85.8868 - val_accuracy: 0.8892\n"
     ]
    }
   ],
   "source": [
    "TRAIN = True\n",
    "\n",
    "if TRAIN:\n",
    "    print(\"\\n---------- TRAINING ----------\\n\")\n",
    "    \n",
    "    \n",
    "    model_id = \"first_model\"\n",
    "    LOG_DIR = os.path.join(\".\", \"_tf_logs\", \"12_tf_data\")\n",
    "    os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "    # Tensorboard\n",
    "    run_index = 3\n",
    "    run_logdir = os.path.join(LOG_DIR, model_id + \"_run_{:02d}\".format(run_index))\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(\n",
    "        log_dir=run_logdir,  histogram_freq=1, profile_batch=10)\n",
    "    print(\"\\n--- TensorBoard Directory: {} ---\\n\".format(run_logdir))\n",
    "\n",
    "    # All callbacks\n",
    "    callbacks = [tensorboard_cb]\n",
    "    \n",
    "    \n",
    "    # Train Model\n",
    "    \n",
    "    model = create_fashion_model(train_set)\n",
    "\n",
    "    optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit(train_set, epochs=10, \n",
    "                        validation_data=valid_set,\n",
    "                        callbacks=callbacks)\n",
    "else:\n",
    "    print(\"\\n---------- LOADING SAVED MODEL ----------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "17db982a-ae81-491b-8ae7-ea8c7b0df8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir=./_tf_logs/12_tf_data --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e50848a-48e4-4bb5-8f45-71b618d0ab1a",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5808ba8f-e4c8-44d2-9cfd-327860a7c205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 39.6910 - accuracy: 0.8773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[39.69100570678711, 0.8773000240325928]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66e8c7f-9763-4006-8b1f-59cc0da1b06f",
   "metadata": {},
   "source": [
    "# IMDB Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eafff63-49f1-4442-862f-1e3e3c576900",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ddfb71d2-a1ad-4eab-ab18-adacfe271d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- ALREADY EXTRACTED ----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "EXTRACTED = True\n",
    "IMDB_PATH = os.path.join(\".\", \"_datasets\", \"imdb_reviews\")\n",
    "\n",
    "def extract_imdb(imdb_path=IMDB_PATH):\n",
    "    tgz_path = os.path.join(imdb_path, \"aclImdb_v1.tar.gz\")\n",
    "    \n",
    "    imdb_tgz = tarfile.open(tgz_path)\n",
    "    imdb_tgz.extractall(path=imdb_path)\n",
    "    imdb_tgz.close()\n",
    "    \n",
    "if not EXTRACTED:\n",
    "    print(\"\\n---------- EXTRACTING DATA ----------\\n\")\n",
    "    \n",
    "    extract_imdb()\n",
    "else:\n",
    "    print(\"\\n---------- ALREADY EXTRACTED ----------\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2afa94c-77f4-4de3-9eac-8f45fbcb5ea3",
   "metadata": {},
   "source": [
    "From the README:\n",
    ">Within these\n",
    "directories, reviews are stored in text files named following the\n",
    "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
    "the star rating for that review on a 1-10 scale. For example, the file\n",
    "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
    "example with unique id 200 and star rating 8/10 from IMDb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "91b15302-00d4-451e-a6f6-5a75096466f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf ./_datasets/imdb_reviews/aclImdb/train/pos/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c781c29-bb34-442f-bc65-06e26b4095fb",
   "metadata": {},
   "source": [
    "### Split Train, Valid, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "0c60f3b2-ec5f-40e3-8b04-5f98a4b2d1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- NOT DEBUGGING ----------\n",
      "\n",
      "\n",
      "--- Train, Valid, and Test Filepaths ---\n",
      "\n",
      "Train:  12500 12500\n",
      "Valid:  7500 7500\n",
      "Test:  5000 5000\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "if DEBUG:\n",
    "    print(\"\\n---------- DEBUGGING ----------\\n\")\n",
    "\n",
    "    TRAIN_POS_DATA_DIR = os.path.join(\".\", \"_datasets\", \"imdb_reviews\", \"aclImdb\", \"train\", \"pos\")\n",
    "    TRAIN_POS_FILENAMES = os.listdir(TRAIN_POS_DATA_DIR)\n",
    "\n",
    "    print(\"\\n--- Train set positive reviews filenames ---\\n\")\n",
    "    print(\"Num files: \", len(TRAIN_POS_FILENAMES))\n",
    "    for filename in TRAIN_POS_FILENAMES[:10]:\n",
    "        print(filename)\n",
    "    print(\"\\n...\\n\")\n",
    "    for filename in TRAIN_POS_FILENAMES[-10:]:\n",
    "        print(filename)\n",
    "\n",
    "    TRAIN_NEG_DATA_DIR = os.path.join(\".\", \"_datasets\", \"imdb_reviews\", \"aclImdb\", \"train\", \"neg\")\n",
    "    TRAIN_NEG_FILENAMES = os.listdir(TRAIN_NEG_DATA_DIR)\n",
    "\n",
    "    print(\"\\n--- Train set negative reviews filenames ---\\n\")\n",
    "    print(\"Num files: \", len(TRAIN_NEG_FILENAMES))\n",
    "    for filename in TRAIN_NEG_FILENAMES[:10]:\n",
    "        print(filename)\n",
    "    print(\"\\n...\\n\")\n",
    "    for filename in TRAIN_NEG_FILENAMES[-10:]:\n",
    "        print(filename)\n",
    "        \n",
    "    print(\"\\n--- Examination of a txt file in Train Pos ---\\n\")\n",
    "    rand_index = np.random.randint(len(TRAIN_POS_FILENAMES)-1)\n",
    "    sample_filename = TRAIN_POS_FILENAMES[rand_index]\n",
    "    sample_filepath = os.path.join(TRAIN_POS_DATA_DIR, sample_filename)\n",
    "    sample_file = tf.io.read_file(sample_filepath)\n",
    "    print(\"Filename: {}\\n\".format(sample_filename))\n",
    "    print(\"File: {}\\n\".format(sample_file))\n",
    "    \n",
    "    print(\"\\n--- Read the entire txt as one element in dataset ---\\n\")\n",
    "    dataset = tf.data.TextLineDataset(sample_filepath)\n",
    "    for item in dataset:\n",
    "        print(item)\n",
    "        \n",
    "else:\n",
    "    print(\"\\n---------- NOT DEBUGGING ----------\\n\")\n",
    "    \n",
    "    \n",
    "    # Get all filepaths for train pos/neg, val pos/neg, and test pos/neg\n",
    "    \n",
    "    def return_filepaths(directory):\n",
    "        return [os.path.join(directory, filename)\n",
    "                for filename in os.listdir(directory)]\n",
    "    \n",
    "    TRAIN_DIR = os.path.join(\".\", \"_datasets\", \"imdb_reviews\", \"aclImdb\", \"train\")\n",
    "    train_pos_filepaths = return_filepaths(os.path.join(TRAIN_DIR, \"pos\"))\n",
    "    train_neg_filepaths = return_filepaths(os.path.join(TRAIN_DIR, \"neg\"))\n",
    "    \n",
    "    TEST_DIR = os.path.join(\".\", \"_datasets\", \"imdb_reviews\", \"aclImdb\", \"test\")\n",
    "    test_pos_filepaths = return_filepaths(os.path.join(TEST_DIR, \"pos\"))\n",
    "    test_neg_filepaths = return_filepaths(os.path.join(TEST_DIR, \"neg\"))\n",
    "    valid_pos_filepaths = test_pos_filepaths[:7500]\n",
    "    valid_neg_filepaths = test_neg_filepaths[:7500]\n",
    "    test_pos_filepaths = test_pos_filepaths[7500:]\n",
    "    test_neg_filepaths = test_neg_filepaths[7500:]\n",
    "    \n",
    "    print(\"\\n--- Train, Valid, and Test Filepaths ---\\n\")\n",
    "    print(\"Train: \", len(train_pos_filepaths), len(train_neg_filepaths))\n",
    "    print(\"Valid: \", len(valid_pos_filepaths), len(valid_neg_filepaths))\n",
    "    print(\"Test: \", len(test_pos_filepaths), len(test_neg_filepaths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3e6e3d-ecf3-403a-b7a4-cc0439411076",
   "metadata": {},
   "source": [
    "### Create Efficient Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a23b7-a4cd-49de-82f4-da6165f0e6a8",
   "metadata": {},
   "source": [
    "- Create a list of ones and zeros for pos and neg filepaths\n",
    "- Create dataset from filepaths and labels\n",
    "- Shuffle dataset\n",
    "- Read each file in the dataset\n",
    "- Batch and prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "1ce2d6ec-6e02-4484-8204-0d265074a395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- NOT DEBUGGING ----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "if DEBUG:\n",
    "    print(\"\\n---------- DEBUGGING ----------\\n\")\n",
    "    \n",
    "    \n",
    "    print(\"\\n--- Train Set Filepaths with Labels ---\\n\")\n",
    "    train_pos_labels = np.ones(len(train_pos_filepaths))\n",
    "    train_neg_labels = np.zeros(len(train_neg_filepaths))\n",
    "    \n",
    "    train_labels = np.r_[train_pos_labels, train_neg_labels]\n",
    "    train_filepaths = train_pos_filepaths + train_neg_filepaths\n",
    "    \n",
    "    train_set = tf.data.Dataset.from_tensor_slices((train_filepaths, tf.cast(train_labels, tf.uint8)))\n",
    "    \n",
    "    for filepath, label in train_set.take(3):\n",
    "        print(\"Filepath: \", filepath)\n",
    "        print(\"Label: \", label)\n",
    "    \n",
    "    print(\"\\n--- Train Set File Text with Labels ---\\n\")\n",
    "    train_set = train_set.shuffle(len(train_set))\n",
    "    \n",
    "    def preprocess(filepath, label):\n",
    "        return tf.io.read_file(filepath), label\n",
    "    \n",
    "    train_set = train_set.map(preprocess,\n",
    "                              num_parallel_calls=AUTO)\n",
    "    for filepath, label in train_set.take(1):\n",
    "        print(\"File: \", filepath)\n",
    "        print(\"Label: \", label)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n---------- NOT DEBUGGING ----------\\n\")\n",
    "    \n",
    "    \n",
    "    def preprocess(filepath, label):\n",
    "        return tf.io.read_file(filepath), label\n",
    "    \n",
    "    def imdb_dataset(pos_filepaths, neg_filepaths):\n",
    "        labels = np.r_[np.ones(len(pos_filepaths)), \n",
    "                       np.zeros(len(neg_filepaths))]\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (tf.constant(pos_filepaths + neg_filepaths),\n",
    "             tf.cast(labels, tf.uint8))\n",
    "        )\n",
    "        dataset = dataset.shuffle(len(train_set))\n",
    "        dataset = dataset.map(preprocess,\n",
    "                              num_parallel_calls=AUTO)\n",
    "        return dataset.batch(32).prefetch(1)\n",
    "    \n",
    "    train_set = imdb_dataset(train_pos_filepaths, train_neg_filepaths)\n",
    "    valid_set = imdb_dataset(valid_pos_filepaths, valid_neg_filepaths)\n",
    "    test_set = imdb_dataset(test_pos_filepaths, test_neg_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fe6136-7ec3-47b3-8960-08d072adfdf5",
   "metadata": {},
   "source": [
    "### Binary Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b695a93a-bc36-4f56-9b8f-ad8d6f940974",
   "metadata": {},
   "source": [
    "- Create a text vectorization layer\n",
    "- Create an embedding layer\n",
    "- Create model with sigmoid activation for output\n",
    "- Train and evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8369baaa-3f1b-4058-b285-c487350b0b19",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### TextVectorization Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b6878c-faa8-4b38-aaf9-f04ef2e511f9",
   "metadata": {},
   "source": [
    "What does tf.keras.layers.TextVectorization do?\n",
    ">A preprocessing layer which maps text features to integer sequences.\n",
    "\n",
    "The processing of each example contains the following steps:\n",
    "- Standardize each example (usually lowercasing + punctuation stripping)\n",
    "- Split each example into substrings (usually words)\n",
    "- Recombine substrings into tokens (usually ngrams)\n",
    "- Index tokens (associate a unique int value with each token)\n",
    "- Transform each example using this index, either into a vector of ints or a dense float vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "90ea3316-6ba8-4601-9620-57c34b113060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'It is a rare occasion when I want to see a movie again. \"The Amati Girls\" is such a movie. In old time movie theaters I would have stayed put for more showings. Was this story autobiographical for the writer/director? It has the aura of reality.<br /><br />The all star cast present their characters believably and with tenderness. Who would not want Mercedes Ruehl as an older sister? I have loved her work since \"For Roseanna\".<br /><br />With most movies, one suspends belief because we know that it is the work of actors, producers, directors, sound technicians, etc. It was hard to suspend such belief in \"The Amati Girls\". One feels such a part of this family! How I wanted to come to the defense of Dolores when her family is stifling her emotional life. And wanted to cheer Lee Grant as she levels criticism at Cloris Leachman\\'s hair color. The humor throughout is not belly laugh humor, but instead has a feel-good quality that satisfies far more than pratfalls and such.<br /><br />The love that is portrayed in this cinema family is to be emulated and cherished.<br /><br />It is no coincidence that the family name, Amati, translated from the Italian means \\'the loved ones\\'.'>"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = next(iter(train_set.unbatch().take(1)))[0]\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "a8b9f5a3-31d0-471e-a91f-c902c604c61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'This movie was so great!<br /><br />I love \"Finding Nemo\" so much!!!'"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_text = b\"This movie was so great!<br /><br />I love \\\"Finding Nemo\\\" so much!!!\"\n",
    "debug_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "44f14c62-aada-4c99-943b-73e39550c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=1000,\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    split=\"whitespace\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=50,\n",
    ")\n",
    "\n",
    "text_vec_layer.adapt([debug_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "a4e47a18-cd57-4c01-9c4f-a2e4ea9b18f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 50), dtype=int64, numpy=\n",
       "array([[ 4,  7,  3,  2, 10, 12,  9,  8, 11,  5,  2,  6,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0]])>"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer([debug_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ff75e-399a-4e50-be8a-93c30a644676",
   "metadata": {},
   "source": [
    "The string has been mapped to a vector of size 50, and each word has been indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "bae913f2-55ab-41c1-ace7-426d344861b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'so',\n",
       " 'was',\n",
       " 'this',\n",
       " 'nemo',\n",
       " 'much',\n",
       " 'movie',\n",
       " 'love',\n",
       " 'i',\n",
       " 'greatbr',\n",
       " 'finding',\n",
       " 'br']"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db5f34a-c95a-459f-8f41-a6399233e696",
   "metadata": {},
   "source": [
    "We need to do our own preprocessing before using keras' TextVectorization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "f6643603-02e0-4180-a25b-aa1ad8347c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_imdb_text(text):\n",
    "    Z = tf.strings.substr(text, 0, 300)\n",
    "    Z = tf.strings.lower(Z)\n",
    "    Z = tf.strings.regex_replace(Z, b\"<br\\\\s*/?>\", b\" \")\n",
    "    Z = tf.strings.regex_replace(Z, b\"[^a-z]\", b\" \")\n",
    "    return [Z.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "2151cb96-9359-478a-bff9-6e285ea49a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'this movie was so great   i love  finding nemo  so much   ']"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_imdb_text(debug_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "e6e8e66b-57b6-410d-bbda-352e89bc7b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=1000,\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    split=\"whitespace\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=50,\n",
    ")\n",
    "\n",
    "text_vec_layer.adapt(preprocess_imdb_text(debug_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "05abe7b1-f00b-4dcd-a4b7-8b6826ac6492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 50), dtype=int64, numpy=\n",
       "array([[ 4,  7,  3,  2, 10,  9,  8, 11,  5,  2,  6,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0]])>"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer(preprocess_imdb_text(debug_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "ac49c7f5-8699-4a1c-b480-a3225871f9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'so',\n",
       " 'was',\n",
       " 'this',\n",
       " 'nemo',\n",
       " 'much',\n",
       " 'movie',\n",
       " 'love',\n",
       " 'i',\n",
       " 'great',\n",
       " 'finding']"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12016cf-28dd-4925-a285-7f6b25d7b9e1",
   "metadata": {},
   "source": [
    "Let's try adapting to a real sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "92ad4d98-240e-485f-a21d-cbbcff05acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "653bbe34-a31f-43f4-af83-80ca07659d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer.adapt(preprocess_imdb_text(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "a7abd3bd-9601-46f6-812c-f87bcddc927d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'a',\n",
       " 'it',\n",
       " 'is',\n",
       " 'i',\n",
       " 'for',\n",
       " 'writer',\n",
       " 'would',\n",
       " 'when',\n",
       " 'was',\n",
       " 'want',\n",
       " 'to',\n",
       " 'time',\n",
       " 'this',\n",
       " 'their',\n",
       " 'theaters',\n",
       " 'such',\n",
       " 'story',\n",
       " 'stayed',\n",
       " 'star',\n",
       " 'showings',\n",
       " 'see',\n",
       " 'reality',\n",
       " 'rare',\n",
       " 'put',\n",
       " 'present',\n",
       " 'old',\n",
       " 'of',\n",
       " 'occasion',\n",
       " 'more',\n",
       " 'in',\n",
       " 'have',\n",
       " 'has',\n",
       " 'girls',\n",
       " 'director',\n",
       " 'characters',\n",
       " 'cast',\n",
       " 'autobiographical',\n",
       " 'aura',\n",
       " 'amati',\n",
       " 'all',\n",
       " 'again']"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "b236075d-e216-4a66-a266-98c04a39885b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 50), dtype=int64, numpy=\n",
       "array([[ 5,  6,  4, 26, 31, 11,  7, 13, 14, 24,  4,  3, 44,  2, 42, 36,\n",
       "         6, 19,  4,  3, 33, 29, 15,  3, 18,  7, 10, 34, 21, 27,  8, 32,\n",
       "        23, 12, 16, 20, 40,  8,  2,  9, 37,  5, 35,  2, 41, 30, 25,  2,\n",
       "        43, 22]])>"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer(preprocess_imdb_text(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aa3fa2-33e3-42c8-bfe7-132758219a12",
   "metadata": {},
   "source": [
    "Let's try making the text preprocessing into a custom layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "033b7759-7c4c-4010-b4cf-bda64e395485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_imdb(X_batch):\n",
    "    Z = tf.strings.lower(X_batch)\n",
    "    Z = tf.strings.regex_replace(Z, b\"<br\\\\s*/?>\", b\" \")\n",
    "    Z = tf.strings.regex_replace(Z, b\"[^a-z]\", b\" \")\n",
    "    return Z\n",
    "\n",
    "class TextPreprocess(keras.layers.Layer):\n",
    "    def __init__(self, dtype=tf.string, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "    def __call__(self, inputs):\n",
    "        return preprocess_imdb(inputs)\n",
    "    \n",
    "text_preprocess_layer = TextPreprocess()\n",
    "\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=1000,\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    split=\"whitespace\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=50,\n",
    ")\n",
    "\n",
    "sample_review_batches = train_set.map(lambda review, label: review)\n",
    "\n",
    "text_vec_layer.adapt(sample_review_batches.take(100))\n",
    "\n",
    "model = keras.Sequential([\n",
    "    text_preprocess_layer,\n",
    "    text_vec_layer,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "fb5fa6f9-ee77-4262-888c-69af5fe7b063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 86ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1,  17,   4, ...,   1,  52,  17],\n",
       "       [ 10, 292,  11, ...,   6, 270, 340],\n",
       "       [  1,   7,   6, ...,   5, 186, 257],\n",
       "       ...,\n",
       "       [ 33,   1, 325, ...,   1,   3,  42],\n",
       "       [  1, 140,   7, ...,  32, 882,   1],\n",
       "       [  2,   1,   7, ...,   2, 193,   1]])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(train_set.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8880c3-fc8f-4766-9664-f5acf1dd562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer(\"text_vectorization_5\").get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d413a7-bd0d-4a88-9080-5b7692264b58",
   "metadata": {},
   "source": [
    "#### Bag of Words Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b440792f-51ad-4a01-863b-427f57da0973",
   "metadata": {},
   "source": [
    "Now let's implement a custom bag of words class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "ca9d472e-dca5-4c50-b993-258c37db890c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_example = tf.constant([[1, 3, 1, 0, 0], [2, 2, 0, 0, 0]])\n",
    "tf.one_hot(simple_example, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "48ed44ef-8d85-4a9e-b3f4-911ab7b1a09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 2.],\n",
       "       [3., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tf.one_hot(simple_example, 2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "f5020847-cffd-4575-ab27-5f7e9a09038f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       "array([[2., 2., 0., 1., 0.],\n",
       "       [3., 0., 2., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tf.one_hot(simple_example, 5), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "de9f6ee4-01ad-4344-986d-c1d57a4bf9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords(keras.layers.Layer):\n",
    "    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        self.n_tokens = n_tokens\n",
    "    def call(self, inputs):\n",
    "        one_hot = tf.one_hot(inputs, self.n_tokens)\n",
    "        return tf.reduce_sum(one_hot, axis=1)[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "9e23da29-52e1-4541-83c4-327bcb84fa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 120ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[15.,  2.,  1., ...,  0.,  0.,  0.],\n",
       "       [21.,  4.,  2., ...,  0.,  0.,  0.],\n",
       "       [14.,  3.,  2., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [ 7.,  2.,  0., ...,  0.,  1.,  0.],\n",
       "       [ 8.,  2.,  1., ...,  0.,  0.,  0.],\n",
       "       [11.,  4.,  1., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_layer = BagOfWords(n_tokens=50)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    text_preprocess_layer,\n",
    "    text_vec_layer,\n",
    "    bow_layer,\n",
    "])\n",
    "\n",
    "model.predict(train_set.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef32991c-f179-413b-9a0c-b4e1c93c19b2",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e426ddea-b5e1-4850-b033-0b5e5f4ca7ab",
   "metadata": {},
   "source": [
    "Classes for text preprocessing and bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "252b2381-ca00-4543-8512-375c15b6c050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_imdb(X_batch):\n",
    "    Z = tf.strings.lower(X_batch)\n",
    "    Z = tf.strings.regex_replace(Z, b\"<br\\\\s*/?>\", b\" \")\n",
    "    Z = tf.strings.regex_replace(Z, b\"[^a-z]\", b\" \")\n",
    "    return Z\n",
    "\n",
    "class TextPreprocess(keras.layers.Layer):\n",
    "    def __init__(self, dtype=tf.string, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "    def __call__(self, inputs):\n",
    "        return preprocess_imdb(inputs)\n",
    "    \n",
    "class BagOfWords(keras.layers.Layer):\n",
    "    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        self.n_tokens = n_tokens\n",
    "    def call(self, inputs):\n",
    "        one_hot = tf.one_hot(inputs, self.n_tokens)\n",
    "        return tf.reduce_sum(one_hot, axis=1)[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "013d0c43-1ca9-4734-9c5e-cfc490ec8337",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_backend()\n",
    "\n",
    "text_preprocess_layer = TextPreprocess()\n",
    "\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=1000,\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    split=\"whitespace\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=50,\n",
    ")\n",
    "\n",
    "sample_review_batches = train_set.map(lambda review, label: review)\n",
    "\n",
    "text_vec_layer.adapt(sample_review_batches.take(100))\n",
    "\n",
    "bow_layer = BagOfWords(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66501ef-30bb-4dc2-97d2-7d4995276f8a",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "3ecc04cd-a990-49a7-b7b5-07cfb10351d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.1714 - accuracy: 0.9512 - val_loss: 2.5331 - val_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "194/782 [======>.......................] - ETA: 4s - loss: 0.5037 - accuracy: 0.8289"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [363]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mNadam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     12\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 14\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_set\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/learningml/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/learningml/lib/python3.10/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/learningml/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/learningml/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/learningml/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/learningml/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/learningml/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/learningml/lib/python3.10/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/learningml/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    text_preprocess_layer,\n",
    "    text_vec_layer,\n",
    "    bow_layer,\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=1e-4)\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_set, epochs=5,\n",
    "                    validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c0e710-96bb-4e2c-b66c-0bcb29e7dd65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
